= ProActive Administration Guide
include::../common.adoc[]
:imagesdir: src/docs/admin/images

== Overview

The administration guide covers cluster setup and cluster administration. Cluster setup includes two main steps

* the installation and configuration of the *scheduler server*
* *computing nodes* setup

image::architecture.png[]

== Get Started

=== What you will need

* http://www.oracle.com/technetwork/java/javase/downloads/index.html[JDK 1.6] or later
* http://www.activeeon.com/community-downloads[Proactive Scheduler]

NOTE: The Proactive Scheduler works with OpenJDK but it is recommended to use JDK from Oracle.

=== Set up from scratch

http://www.activeeon.com/community-downloads[Download] Proactive Scheduler and *unzip* the archive.

The extracted folder will be referenced as +PROACTIVE_HOME+ in the rest of the documentation.
The distribution contains all required dependencies.

=== Run the Proactive Scheduler

Proactive Scheduler is ready to be started with no extra configuration.

[source]
----
$ cd PROACTIVE_HOME/bin
----

----
$ jrunscript start-server.js
----

TIP: If the command +jrunscript+ cannot be found, http://docs.oracle.com/javase/7/docs/webnotes/install/windows/jdk-installation-windows.html#path[check that the +PATH+ environment variable contains the JDK executables].

When started, Scheduler, Resource Manager and Web portals URLs are displayed.

[source]
----
---------------------------------
    Starting server processes
---------------------------------

Running Resource Manager process ...
> Starting the resource manager...
> The resource manager with 4 local nodes created on pnp://hostname.local:64738/
Resource Manager stdout/stderr redirected into /home/user/proactive_scheduling/.logs/RM-stdout.log

Running Scheduler process ...
> RM URL : pnp://localhost:64738
> Starting the scheduler...
> Connecting to the resource manager on pnp://localhost:64738
> The scheduler created on pnp://hostname.local:52845/
Scheduler stdout/stderr redirected into /home/user/proactive_scheduling/.logs/Scheduler-stdout.log

Running Jetty process ...
Jetty stdout/stderr redirected into /home/user/proactive_scheduling/.logs/Jetty-stdout.log
Waiting for jetty to start ...
Rest Server webapp deployed at      http://localhost:8080/rest
Resource Manager webapp deployed at http://localhost:8080/rm
Scheduler webapp deployed at        http://localhost:8080/scheduler

Preparing to wait for processes to exit ...
Hit CTRL+C or enter 'exit' to terminate all server processes and exit
----

As presented in the <<_overview>>, all the ProActive Scheduler components are started:

 * *Resource Manager* that handles all nodes on a cluster
 * *Scheduler* that accepts jobs from users and orders them
 * *REST API* for the Resource Manager and Scheduler
 * *Web portals* for the Resource Manager and Scheduler
 * *Workflow Studio* that is the web interface for designing you distributed computations.
 * 4 local *computing nodes* (JVM)

Now your Proactive Scheduler is ready to execute jobs!


== Scheduler configuration

All configuration files of Proactive Scheduler can be found inside the distribution.

.ProActive configuration files
|===
|Component | Description|File |Reference

.1+^.^|Scheduler
|Scheduling Properties|config/scheduler/settings.ini|<<TODO>>

.1+^.^|Resource Manager
|Node management configuration|config/scheduler/settings.ini|<<TODO>>

.1+^.^|Networking
|Network, firewall, protocols configuration|config/proactive/ProActiveConfiguration.xml|<<TODO>>

.4+^.^|Security
|User logins and passwords|config/authentication/login.cfg|<<TODO>>
|User group assignments|config/authentication/group.cfg|<<TODO>>
|User permissions|config/security.java.policy-server|<<TODO>>
|LDAP configuration|config/authentication/ldap.cfg|<<TODO>>

|===


== Installation on a Cluster

Adding machines of a cluster to the ProActive Scheduler typically involves unpacking the software on all those host machines.
Once it's done you need to run a computing node on the target machine and connect it to the scheduler. There are two principal ways of
doing that:

* Launch a process on the remote side and connect it to the server
* Initiate the deployment from the server side - http://tbd[node source creation]

If you are not familiar with Proactive Scheduler you may want to try the *first method* as it's easier to understand.
With the combination of http://tbd[Proactive Agent] it gives you the same result.

The *second method* implies that you have an access to remote hosts (e.g. SSH access) and you want to start and stop computing nodes
by launching commands over one of the protocols we support. For instance it can be useful when before launching a computing node you'd
like to deploy a virtual machine.

[[run_node_manually]]
=== Run a node manually

Let's take a closer look at the first method described above. To deploy a node from the remote machine you need
to run the following command
----
$ PROACTIVE_HOME/bin/unix/rm-start-node -r pnp://localhost:64738
----
where +-r+ option is used to specify the url of the *resource manager*. You can find this url in the output of the server launcher.

It is also possible to launch a node without even copying proactive to a remote machine. To to is you need to:

* Open a browser on the remote host
* Go to the http://SCHEDULER_ADDRESS:8080/rm[resource manager portal]
* Use default demo/demo account to access the portal
* Click on 'Portal->Launch' to download *node.jar*
* Run it
----
$ java -jar node.jar
----

It should connect to the scheduler automatically.

TIP: Launch several processes if you would like to execute several computations at the same time on one host. Each
node process allows to execute one task at the same time.

=== Deploy nodes through SSH

The second way of deploying nodes is to create *node sources* on the server side.

NOTE: A *Node Source* is a set of nodes running on the same infrastructure and having the same access policy.
E.g. a cluster with ssh access where nodes are available from 9 a.m. to 9 p.m. Or another example: nodes
from Amazon EC2 available permanently for users from group 'cloud'.

When a node source is created you can choose an *infrastructure manager* from one of supported http://tbd[infrastructures]
 and a http://tbd[policy] that defines rules and limitations of nodes' utilization.

To create a node source you can either:

* Use the web interface of the *resource manager* ('Add Nodes' menu)
* Use the http://tbd[REST API]
* Use the http://tbd[command line client] inside the distribution

In order to create an SSH node source you should first configure an *SSH access* from the server to the computing nodes
that http://www.linuxproblem.org/art_9.html[does not require password]. Then create a text file where you list all
your computing nodes and number of jobs that can be executed in parallel, e.g.

[source]
----
# you can use network names
host1.mydomain.com 2
host2.mydomain.com 4
# or ip addresses
192.168.0.10 8
192.168.0.11 16
----

Then using this file create a node source either from the *Resource Manager web interface* or from the command line
[source]
----
$ cd PROACTIVE_HOME
$ ./bin/unix/rm-client -createns ssh_node_source -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.SSHInfrastructure rmi://hostname:1099/ hosts_file 60000 3 \"\" /home/user/jdk/bin/java PROACTIVE_HOME Linux \"\" config/authentication/rm.cred
----

TODO link to reference to explain parameters

=== Agents

In your production environment you might want to control and limit the resources utilization for all hosts in your network,
especially if these are desktop machines where people perform their daily activities. Using *Proactive Agent* you can:

* Control the number of ProActive nodes on each machine
* Launch nodes automatically when a machine starts
* Restart JVM processes if they fail by some reason and reconnect them to Proactive Scheduler

*Agents* exists for both +Linux+ and +Windows+ operating systems.

==== Windows Agent

The ProActive Windows Agent is a *Windows Service*: a long-running executable that performs specific
functions and which is designed to not require user intervention. The agent is able to create a ProActive
computational resource on the local machine and to connect it to the *Proactive Resource Manager*.

After being installed, it:

* Loads the user's configuration
* Creates schedules according to the working plan specified in the configuration
* Spawns a JVM process that will run a specified java class depending on the selected connection type. 3 types of connections are available:
** *Local Registration* - The specified java class will create a ProActive local node as a computational resource and register it locally.
** *Resource Manager Registration* - The specified java class will create a ProActive local node as a computational resource and register
it in the specified Resource Manager, thus being able to execute java or native tasks received from the Scheduler. It is is important
to note that a JVM process running tasks can potentially spawn child processes.
** *Custom* - The user can specify his own java class.
* Watches the spawned JVM process in order to comply with the following limitations:
** *RAM limitation* - The user can specify a maximum amount of memory allowed for a JVM process and its children. If the limit is reached, then all processes are automatically killed.
** *CPU limitation* - The user can specify a maximum CPU usage allowed for a JVM process and its children. If the limit is exceeded by the sum of CPU usages of all processes, they are automatically throttled to reach the given limit.
* Restarts the spawned JVM process in case of failures with a timeout policy.

==== Install Agents on Windows

The ProActive Windows Agent installation pack is available on the official http://proactive.inria.fr/[ProActive website].
Run the +setup.exe+ file and follow instructions. When the following dialog appears:

image::install_config.png[align="center"]

. Specify the directory that will contain the *configuration* file named +PAAgent-config.xml+, note that if this file already exists in the specified directory it will be re-used.
. Specify the directory that will contain the *log files* of the ProActive Agent and the spawned runtimes.
. Specify an existing, local *account* under which the ProActive Runtime(s) will be spawned. It is highly recommended to specify an account that is not part of the Administrators group to isolate the ProActive Runtime and reduce security risks.
. The *password* is encrypted using Microsoft AES Cryptographic Provider and only Administrators have access permissions to the keyfile (restrict.dat) this is done using the SubInACL tool.
. If the specified account does not exist the installation program will prompt the user to create a non-admin account with the required privileges.
+
Note that the ProActive Agent service is installed under LocalSystem account, this should not be changed,
however it can be using the +services.msc+ utility. ('Control Panel->Administrative Tools->Services')

. If you want that any non-admin user (except guest accounts) be able to start/stop the ProActive Agent service check the "Allow everyone to start/stop" box. If this option is checked the installer will use the SubInACL tool. If the tool is not installed in the +Program Files\Windows Resource Kits\Tools+ directory the installer will try to download its installer from the official Microsoft page.
. The installer will check whether the selected user account has the required privileges. If not follow the steps to add these privileges:
.. In the 'Administrative Tools' of the 'Control Panel', open the 'Local Security Policy'.
.. In 'Security Settings', select 'Local Policies' then select 'User Rights Assignments'.
.. Finally, in the list of policies, open the properties of 'Replace a process-level token' policy and add the needed user. Do the same for 'Adjust memory quotas for a process'. For more information about these privileges refer to the official Microsoft page.

At the end of the installation, the ProActive Agent Control utility should be started. This next section explains how to configure it.

To uninstall the ProActive Windows Agent, simply run 'Start->Programs->ProActiveAgent->Uninstall ProActive Agent'.

==== Configure Agents on Windows

To configure the Agent, launch 'Start->Programs->ProActiveAgent->AgentControl' program or click on
the notify icon if the "Automatic launch" is activated.
Double click on the tray icon to open the *ProActive Agent Control* window. The following window will appear:

image::agent_control.png[align="center"]

From the ProActive Agent Control window, the user can load a configuration file, edit it, start/stop the service and view logs.
A GUI for editing is provided (explained below). Even if it is not recommended, you can edit the configuration file by yourself with your favorite text editor.

It is also possible to change the ProActive Runtime Account using the 'Change Account' button.

When you click on 'GUI Edit', the following window appears:

image::config_editor_general.png[align="center"]

In the general tab, the user can specify:

* The *ProActive Scheduler* location.
* The *JRE* location (usually something like +C:\Program Files\Java\jdk1.6.0_12+).
* The *number* of Runtimes (the number of spawned JVMs).
* The *JVM options*.
* Note that if the parameter contains +${rank}+, it will be dynamically replaced by the Runtime rank starting from 0.
* The *On Runtime Exit* script. A script executed after a Runtime exits. This can be useful to perform additional cleaning operation.
* Note that the script receives as parameter the PID of the Runtime.
* The user can set a *memory limit* that will prevent the spawned processes to exceed a specified amount of RAM. If a spawned process or its child process requires more memory, it will be killed as well as its child processes.
* Note that this limit is disabled by default (0 means no limit) and a ProActive Runtime will require at least 128 MBytes.
* It is possible to list all available *network interfaces* by clicking on the "Refresh" button and add the selected network interface name as a value of the +proactive.net.interface+ property by clicking on "Use" button. See the ProActive documentation for further information.
* The user can specify the *protocol* (rmi or http) to be used by the Runtime for incoming communications.
* To ensure that a unique port is used by a Runtime, the initial port value will be incremented for each node process and given as value of the +-Dproactive.SELECTED_PROTOCOL.port+ JVM property. If the port chosen for a runtime is already used, it is incremented until an available port number is found.

Clicking on the 'Connection' tab, the window will look like this:

image::config_editor_connection.png[]

In the 'Connection' tab, the user can select between three types of connections:

* *Local Registration* - creates a local ProActive node and registers (advertises) it in a local RMI registry. The node name is optional.
* *Resource Manager Registration* - creates a local ProActive node and registers it in the specified Resource Manager. The mandatory Resource Manager's url must be like *protocol://host:port/*. The node name and the node source name are optional. Since the Resource Manager requires authentication, the user specifies the file that contains the credential. If no file is specified the default one located in +%USERPROFILE%\.proactive\security+ folder is used.
* *Custom* - the user specifies his own java starter class and the arguments to be given to the main method. The java starter class must be in the classpath when the JVM process is started.

Finally, clicking on the "Planning" tab, the window will look like this:

image::config_editor_planning.png[]

In the Planning Tab, depending on the selected connection type, the agent will initiate it according to a *weekly planning* where each plan specifies the connection start time as well as the working duration. The agent will end the connection as well as the ProActive Runtime process and its child processes when the plan duration has expired.

Moreover, it is possible to specify the JVM process Priority and its *CPU usage* limit. The behavior of the CPU usage limit works as follows: if the JVM process spawns other processes, they will also be part of the limit so that if the sum of CPU% of all processes exceeds the user limit they will be throttled to reach the given limit. Note that if the Priority is set to RealTime the CPU % throttling will be disabled.

The "Always available" makes the agent to run permanently with a Normal Priority and Max CPU usage at 100%.

==== Launching Windows Agent

Once you have configured the agent, you can start it clicking on the "Start" button of the ProActive Agent Control window. However, before that, you have to ensure that *Proactive Scheduler* has been started on the address you specified in the agent configuration. You do not need to start a node since it is exactly the job of the agent.

Once started, you may face some problems. You can realise that an error occurred by first glancing at the color of the agent tray icon. If everything goes right, it should keep the blue color. If its color changes to yellow, it means that the agent has been stopped. To see exactly what happened, you can look at the runtime log file located into the agent installation directory and named +Executor<runtime number>Process-log.txt+.

The main troubles you may have to face are the following ones:

* You get an *access denied error*: this is probably due to your default java.security.policy file which cannot be found. If you want to specify another policy file, you have to add a JVM parameter in the agent configuration. A policy file is supplied in the scheduling directory. To use it, add the following line in the JVM parameter box of the agent configuration (Figure 5.3, “Configuration Editor window - General Tab ”):
----
-Djava.security.policy=PROACTIVE_HOME/config/security.java.policy-client
----
* You get an *authentication error*: this is probably due to your default credentials file which cannot be found. In the "Connection" tab of the Configuration Editor (Figure 5.4, “Configuration Editor window - Connection Tab (Resource Manager Registration)”), you can choose the credentials file you want. You can select, for instance, the credentials file located at PROACTIVE_HOME/config/authentication/scheduler.cred or your own credentials file.
* The node seems to be well started but you cannot see it in the Resource Manager interface : in this case, make sure that the *port number* is the good one. Do not forget that the runtime port number is incremented from the initial resource manager port number. You can see exactly on which port your runtime has been started looking at the log file described above.


==== Linux Agent

To install the ProActive Agent on Linux Debian based distributions:

* Download the package from our http://www.activeeon.com/community-downloads[website]
* Install the package

    sudo dpkg -i proactive-agent-standalone.deb

By default the Linux Agent will launch locally a node but will not register it
in the Resource Manager. The user has to configure the agent explicitly by specifying the required
parameters in the “config.xml” file.

The Agent's logs are located in +/var/log/proactive/agent/agent.log+.

===== Configure the Linux Agent

To configure the agent behavior:

* Stop the Linux Agent
+
----
sudo /etc/init.d/proactive-agent stop
----

* Update the *config.xml* file in +/opt/proactive-agent+ folder
+
In case there is no such file directory, create the directory. Create the *config.xml* file or create a symbolic link
+
----
sudo mkdir -p /opt/proactive-agent
sudo ln -f -s <path-to-config-file> /opt/proactive-agent/config.xml
----

* Change the ownership of the “config.xml” file
+
----
sudo chown -Rv proactive:proactivegroup config.xml
----
+
If the “proactivegroup” group does not exist, create the group and add “proactive” user to the group:
+
----
sudo groupadd proactivegroup
sudo usermod -g proactivegroup proactive
----

* Start the Linux Agent

    sudo /etc/init.d/proactive-agent start

===== Uninstall the Linux Agent

    sudo dpkg -r proactive-agent

== Installation on a Cluster with Firewall

When the configuration of the network is unfriendly (ports closed, firewalls,...) the Proactive Scheduler allows
you to connect nodes without significant changes in your network firewall configuration.
It relies on the *PAMR* protocol (Proactive Message Routing Protocol) that has the weakest expectations on
how the network is configured.

Unlike all the other communication protocols, PAMR does *not* expect *bidirectional* TCP connections.
It has been designed to work when only outgoing TCP connections are available. Such environments can be encountered due to:

* Network address translation devices
* Firewalls allowing only outgoing connection (this is the default setup of many personal firewalls)
* Virtual Machines with a virtualized network stack

image::firewall.png[]

When PAMR is activated, the ProActive Scheduler and nodes connect to a PAMR router. This connection is kept open, and used
as a tunnel to receive incoming messages. If the tunnel goes down, it is automatically reopened by nodes.

The biggest drawback of PAMR is that a centralized PAMR router is in charge of routing message between all the PAMR clients.
To soften this limitation PAMR can be used with other communication protocols. This way, PAMR is used only when needed.
See http://hudson.activeeon.com/job/ProActive_5.4.x/lastSuccessfulBuild/artifact/doc/built/Programming/ReferenceManual/single_html/ProActiveProgrammingReferenceManual.html#netconfig-multiprotocol[Enabling several communication protocols].
TODO avoid link to external doc

To enable this behavior, you need to edit +PROACTIVE_HOME/config/proactive/ProActiveConfiguration.xml+.
The config file should look like the following:

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<ProActiveUserProperties>
    <properties>
        <prop key="proactive.net.nolocal" value="true"/>
        <prop key="proactive.hostname" value="hostname"/>
        <prop key="proactive.communication.protocol" value="pamr"/>
        <prop key="proactive.net.router.address" value="router.hostname"/>
        <prop key="proactive.net.router.port" value="8090"/>
    </properties>
    <javaProperties>
    </javaProperties>
</ProActiveUserProperties>
----

Use the same configuration file for all computing nodes. Typically you will run the scheduler server on the *same machine* where the router is
launched. This sample configuration requires to open only one port +8090+ for *incoming connections* on the router host
and all the computing nodes will be able to connect to the Scheduler.

Now in order to run the Scheduler you should first run Proactive Router:

    $ cd PROACTIVE_HOME/bin
    $ bash unix/start-router &
    $ jrunscript start-server.js

== Control the resource usage
=== Policies

You can limit the utilization of resources connected to the Proactive Scheduler in different ways. When you create node
sources you can use a node source *policy*.

NOTE: *Node source policy* is a set of rules and conditions which describes when and how many nodes have to be selected for computations.

Each node source policy regardless it specifics has a common part where you describe users' and groups' permissions.
When you create a policy you must specify them:

* *nodeUsers* - utilization permission that defines who can get nodes for computations from this node source. It has to take one of the following values:
** +ME+ - only the node source creator
** +users=user1,user2;groups=group1,group2;tokens=t1,t2+ - only specific users, groups or tokens. I.e. users=user1 - node access is limited to user1; users=user1;groups=group1 - node access is limited to user1 and all users from group group1; users=user1;tokens=t1 - node access is limited to user1 or anyone who specified token t1. If node access is protected by a token, node will not be found by the resource manager (getNodes request) unless the corresponding token is specified.
** +ALL+ - everybody can use nodes from this node source

* *nodeProviders* - provider permission defines who can add nodes to this node source. It should take one of the following values:
** +ME+ - pnly the node source creator
** +users=user1,user2;groups=group1,group2+ - only specific users or groups (for our example user1, user2, group1 and group2). It is possible to specify only groups or only users.
** +ALL+ - everybody can add nodes to this node source

The user who created the node source is the administrator of this node source. He can add and removed nodes to it, remove the node source itself,
but cannot use nodes if usage policy is set to *PROVIDER* or *PROVIDER_GROUPS*.

In the resource manager, there is always a default node source configured with a *DefaultInfrastructureManager* and a *Static policy*.
It is not able to deploy nodes anywhere but makes it possible to add existing nodes to the scheduler (see <<run_node_manually>>)

Out of the box the Scheduler supports time slot policies, cron policies, load based policies and many others.
Please see detailed information about policies in http://tbs[References section]. TODO

=== Agents schedule

*Node source policies* limit computing nodes utilization on the level of the Proactive Scheduler.
If you need fine-grained limits on the node level *Proactive Agents* will help you achieve that.

TIP: The typical scenario is when you use desktop workstation for computations during non working hours.

Both linux and windows agents have an ability to:

* Run node daemons according to the schedule
* Limit resources utilization for these daemons (e.g CPU, memory)

Agents configuration is detailed in the section <<_agents>>.

== User Authentication

In order to use Proactive Scheduler every user must have an account. It supports two methods for authentication:

* *File based*
* *LDAP*

=== Select authentication method

By default the Proactive Scheduler is configured to use *file based* authentication and has some default accounts ('demo/demo', 'admin/admin') that
work out of the box.

If you would like to use your *LDAP* server you need to modify two configs:

* *Resource Manager* configuration (+PROACTIVE_HOME/config/rm/settings.ini+)
+
[source]
----
#Property that defines the method that has to be used for logging users to the Resource Manager
#It can be one of the following values:
#    - "RMFileLoginMethod" to use file login and group management
#    - "RMLDAPLoginMethod" to use LDAP login management
pa.rm.authentication.loginMethod=RMLDAPLoginMethod
----

* *Scheduler* configuration (+PROACTIVE_HOME/config/scheduler/settings.ini+)
+
[source]
----
#Property that define the method that have to be used for logging users to the Scheduler
#It can be one of the following values :
#	- "SchedulerFileLoginMethod" to use file login and group management
#	- "SchedulerLDAPLoginMethod" to use LDAP login management
#	- "SchedulerLDAP2LoginMethod" to use improved LDAP login management
pa.scheduler.core.authentication.loginMethod=SchedulerLDAPLoginMethod
----


=== File

By default, the resource manager stores users accounts, passwords, and group memberships (user or admin), in two files:

* users and passwords accounts are stored in +PROACTIVE_HOME/config/authentication/login.cfg+.
Each line has to follow the format *user:passwd*. The default +login.cfg+ file is given hereafter:
+
[source]
----
admin:admin
user:pwd
demo:demo
----

* users membership is stored in +PROACTIVE_HOME/config/authentication/group.cfg+. For each user registered in login.cfg,
a group membership has to be defined in this file. Each line has to look like *user:group*. Group has to be
user to have user rights, or admin to have administrator rights. Below is an extract of +group.cfg+:
+
[source]
----
admin:admin
demo:admin
user:user
----

=== LDAP

The resource manager is able to connect to an existing *LDAP* server, to check users login/password and verify users group
membership. This authentication method can be used with existing LDAP server that is already configured.


In order to use it, few parameters have to be configured, such as *path in LDAP tree users*, LDAP *groups* that define
user and admin group membership, *URL* of the LDAP server, LDAP *binding method* used by connection and configuration
of SSL/TLS if you want a secured connection between the resource manager and LDAP.

We assume that LDAP server is configured in the way that:

* all existing users and groups are located under single domain
* users have object class specified in parameter *pa.ldap.user.objectclass*
* groups have object class specified in parameter *pa.ldap.group.objectclass*
* user and group name is defined in cn (Common Name) attribute

[source]
----
# EXAMPLE of user entry
#
# dn: cn=jdoe,dc=example,dc=com
# cn: jdoe
# firstName: John
# lastName: Doe
# objectClass: inetOrgPerson

# EXAMPLE of group entry
#
# dn: cn=mygroup,dc=example,dc=com
# cn: mygroup
# firstName: John
# lastName: Doe
# uniqueMember: cn=djoe,dc=example,dc=com
# objectClass: groupOfUniqueNames
----

The LDAP configuration is defined in +PROACTIVE_HOME/config/authentication/ldap.cfg+. You need to:

. *Set the LDAP server URL*
+
First, you have to define the LDAP's URL of your organisation. This address corresponds to the property: +pa.ldap.url+. You have to put a standard LDAP-like URL, for example *ldap://myLdap*. You can also set an URL with secure access: *ldaps://myLdap:636*.
+
. *Define object class of user and group entities*
+
Then you need to define how to differ user and group entities in LDAP tree. The users object class is defined by
property +pa.ldap.user.objectclass+ and by default is _inetOrgPerson_. For groups, the property +pa.ldap.group.objectclass+
has a default value _groupOfUniqueNames_ which could be changed.

. *Configure LDAP authentication parameters*
+
By default, the Proactive Scheduler binds to LDAP in anonymous mode. You can change this authentication
method by modifying the property +pa.ldap.authentication.method+. This property can have several values:
+
* none (default value) - the resource manager performs connection to LDAP in anonymous mode.
* simple - the resource manager performs connection to LDAP with a specified login/password (see below for user password setting).
+
You can also specify a SASL mechanism for LDAPv3. There are many SASL available mechanisms: cram-md5, digest-md5, kerberos4. Just set this property to *sasl* to let the resource manager JVM choose SASL authentication mechanism.
If you specify an authentication method different from 'none' (anonymous connection to LDAP), you must specify a login/password for authentication.
+
There are two properties to set in LDAP configuration file:
+
** +pa.ldap.bind.login+ - sets user name for authentication.
** +pa.ldap.bind.pwd+ - sets password for authentication.
+
. *Set SSL/TLS parameters*
+
A secured SSL/TLS layer can be useful if your network is not trusted, and critical information is transmitted between the rm server and LDAP, such as user passwords.
First, set the LDAP URL property +pa.ldap.url+ to a URL of type *ldaps://myLdap*. Then set +pa.ldap.authentication.method+ to *none* so as to delegate authentication to SSL.
+
For using SSL properly, you have to specify your certificate and public keys for SSL handshake. Java stores certificates in a keyStore and public keys in a trustStore. In most of the cases, you just have to define a trustStore with public key part of LDAP's certificate. Put certificate in a keyStore, and public keys in a trustStore with the keytool command (keytool command is distributed with standard java platforms):
+

    keytool -import -alias myAlias -file myCertificate -keystore myKeyStore
+
myAlias is the alias name of your certificate, myCertificate is your private certificate file and myKeyStore is the new keyStore file produced in output. This command asks you to enter a password for your keyStore.
+
Put LDAP certificate's public key in a trustStore, with the keytool command:
+
    keytool -import -alias myAlias -file myPublicKey -keystore myTrustStore
+
myAlias is the alias name of your certificate's public key, myPublicKey is your certificate's public key file and myTrustore is the new trustStore file produced in output. This command asks you to enter a password for your trustStore.
+
Finally, in +config/authentication/ldap.cfg+, set keyStore and trustStore created before to their respective passwords:
+
* Set +pa.ldap.keystore.path+ to the path of your keyStore.
* Set +pa.ldap.keystore.passwd+ to the password defined previously for keyStore.
* Set +pa.ldap.truststore.path+ to the path of your trustStore.
* Set +pa.ldap.truststore.passwd+ to the password defined previously for trustStore.

. *Use fall back to file authentication*
+
You can use simultaneously file-based authentication and LDAP-based authentication. Then Proactive Scheduler can check user password and group membership in
login and group files, as performed in FileLogin method, if user or group is not found in LDAP.
It uses +pa.rm.defaultloginfilename+ and +pa.rm.defaultgroupfilename+ files to authenticate user and check group membership. There are two rules:

* If LDAP group membership checking fails, fall back to group membership checking with group file.
To activate this behavior set +pa.ldap.group.membership.fallback+ to true, in LDAP configuration file.
* If a user is not found in LDAP, fall back to authentication and group membership checking with login
and group files. To activate this behavior, set +pa.ldap.authentication.fallback+ to true, in LDAP configuration file.

== User Permissions

All users authenticated in the resource manager have they own role according to granted permissions.
In Proactive Scheduler, we use the standard
http://www.oracle.com/technetwork/java/javase/jaas/index.html[Java Authentication and Authorization Service (JAAS)]
to address these needs.

The file +PROACTIVE_HOME/config/security.java.policy-server+ allows to configure fine-grained access for all users, e.g. who has the right to:

* Deploy nodes
* Execute jobs
* Pause the scheduler
* etc

== Monitor the cluster state

Cluster monitoring typically means checking that all nodes that were added to *Proactive Scheduler*
are up and running. We don't track for example the free disk space or software upgrade which can be
better achieved with tools like http://www.nagios.org[Nagios].

In the *resource management* interface of Proactive Scheduler you can see how many nodes were added to the
scheduler and their usage.

image::admin_web.png[]

The same information is accessible using the command line:

[source]
----
$ cd PROACTIVE_HOME
$ ./bin/unix/rm-client --listnodes
----

=== Node States

When you look at your cluster computing nodes can be in one of the following states

* +Deploying+ - The deployment of the node has been triggered by the resource manager but it has not yet been added.
* +Lost+ - The deployment of the node has failed for some reason. The node has never been added to the resource manager and won't be usable.
* +Configuring+ - Node has been added to the resource manager and is being configured. The resource manager computes several pieces information about the node. This step can be time consuming depending TODO finish the sentence.
* +Free+ - Node is available for computations.
* +Busy+ - Node has been given to user to execute computations.
* +Locked+ - Node is under maintenance and cannot be used for computations.
* +To be removed+ - Node is busy but requested to be removed. So it will be removed once the client will release it.
* +Down+ - Node is unreachable or down and cannot be used anymore.


=== JMX

The JMX interface for remote management and monitoring provides information about the running ProActive Resource Manager and allows the user to modify its configuration.
For more details about JMX concepts, please refer to official documentation about
the http://www.oracle.com/technetwork/java/javase/tech/javamanagement-140525.html[JMX architecture].

image::jmx_archi.png[align="center"]

The following aspects (or services) of the *Proactive Scheduler* are instrumented using MBeans that are managed through a JMX agent.

* *Server status* is exposed using the RuntimeDataMBean
** The Resource Manager status
** Available/Free/Busy/Down nodes count
** Average activity/inactivity percentage
* The *Accounts Manager* exposes accounting information using the MyAccountMBean and AllAccountsMBean
** The used node time
** The provided node time
** The provided node count
* Various *management operations* are exposed using the ManagementMBean
** Setting the accounts refresh rate
** Refresh all accounts
** Reload the permission policy file

MBean server can be accessed by remote applications using one of the two available connectors

* The standard solution based on Remote Method Invocation (RMI) protocol is the RMI Connector accessible at the following url:
+service:jmx:rmi:///jndi/rmi://HOSTNAME:PORT/JMXRMAgent+ where
** *HOSTNAME* is the hostname on which the RM is started
** *PORT* (5822 by default) is the port number on which the JMX RMI connector server has been started. It is defined by the property pa.rm.jmx.port .
* The ProActive Remote Objects Connector provides ProActive protocol aware connector accessible at the following url:
+service:jmx:ro:///jndi/PA_PROTOCOL://HOSTNAME:PORT/JMXRMAgent+ where
** *PA_PROTOCOL* is the protocol defined by the proactive.communication.protocol property
** *HOSTNAME* is the hostname on which the RM is started
** *PORT* is the protocol dependent port number usually defined by the property proactive.PA_PROTOCOL.port

The name of the connector (JMXRMAgent by default) is defined by the property +rm.jmx.connectorname+.

The JMX url to connect to can be obtained from the Authentication API of the RM or by reading the log file located in +PROACTIVE_HOME/.logs/RM.log+.
In that log file, the address you have to retrieve is the one where the JMX RMI connector server has been started

[source]
----
[INFO 2010-06-17 10:23:27,813] [RM.AbstractJMXHelper.boot] Started JMX RMI connector server at service:jmx:rmi:///jndi/rmi://kisscool.inria.fr:5822/JMXRMAgent
----

Once connected, you'll get an access to RM statistics and accounting.

For example, to connect to the Proactive Scheduler JMX Agent with JConsole tool, just enter the url of the standard RMI
Connector, as well as the username and the password.

image::jmx_jconsole_connect.png[align="center"]

Then depending on the allowed permissions browse the attributes of the MBeans.

image::jmx_jconsole.png[align="center"]

=== Accounting

The users of Proactive Scheduler request and offer nodes for computation. To keep track of how much node time was
consumed or contributed by a particular user, Proactive Scheduler associates a user to an account.

More precisely, the nodes can be manipulated by the following basic operations available to the users

* The +ADD+ operation is a registration of a node in the Resource Manager initiated by a user considered as a node provider.
A node can be added, through the API, as a result of a deployment process, through an agent or manually from the command line interface.
* The +REMOVE+ operation is the unregistration of a node from the Resource Manager.
A node can be removed, through the API, by a user or automatically if it is unreachable by the Resource Manager.
* The +GET+ operation is a node reservation, for an unknown amount of time, by a user considered as a node owner.
For example, the ProActive Scheduler can be considered as a user that reserves a node for a task computation.
* The +RELEASE+ operation on a reserved node by any user.

The following accounting data is gathered by the Resource Manager

* *The used node time*: The amount of time other users have spent using the resources of a particular user.
More precisely, for a specific node owner, it is the sum of all time intervals from +GET+ to +RELEASE+.
* *The provided node time*: The amount of time a user has offered resources to the Resource Manager.
More precisely, for a specific node provider, it is the sum of all time intervals from +ADD+ to +REMOVE+.
* *The provided node count*: The number of provided nodes.

The accounting information can be accessed only through a *JMX client* or the resource manager *command line*.


== Troubleshooting
=== Logs

If something goes wrong the first place to look for the problem are server logs. By default all logs are in
+PROACTIVE_HOME/.logs+ directory :

* for problems related to nodes deployment or permissions please check
+PROACTIVE_HOME/.logs/RM.log+
* for the problems related to jobs execution please check +PROACTIVE_HOME/.logs/Scheduler.log+

Users submitting jobs have access to server logs of their jobs through the *Scheduler Web interface*

image::server_logs.png[align="center"]

== Reference

=== Scheduler Properties

[source]
----
# Scheduler home directory (this default value should be proper in most cases)
pa.scheduler.home=.

# Timeout for the scheduling loop (in millisecond)
pa.scheduler.core.timeout=2000

# Auto-reconnection to the Resource Manger
pa.scheduler.core.rmconnection.autoconnect = true
pa.scheduler.core.rmconnection.timespan = 5000
pa.scheduler.core.rmconnection.attempts = 20

# Number of threads used to execute client requests
pa.scheduler.core.clientpoolnbthreads=5

# Number of threads used to execute internal scheduling operations
pa.scheduler.core.internalpoolnbthreads=5

# Check for failed node frequency (in second)
pa.scheduler.core.nodepingfrequency=20

# Cache classes definition in task class servers
pa.scheduler.classserver.usecache=true;

# Temporary directory for jobclasspathes
# pa.scheduler.classserver.tmpdir=TO/BE/SET;

# Scheduler default policy full name
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.DefaultPolicy

# Defines the maximum number of tasks to be scheduled in each scheduling loop.
pa.scheduler.policy.nbtaskperloop=10

# Forked java task default security policy path (use to define the policy of the forked task)
pa.scheduler.forkedtask.security.policy=config/scheduler/forkedJavaTask/forkedTask.java.policy

# Log4J forked java task default file path
pa.scheduler.forkedtask.log4j=config/scheduler/forkedJavaTask/forkedTask-log4j

# ProActiveConfiguration forked java task default file path
pa.scheduler.forkedtask.paconfig=config/scheduler/forkedJavaTask/forkedTask-paconf.xml

#Name of the JMX MBean for the scheduler
pa.scheduler.core.jmx.connectorname=JMXSchedulerAgent

# port of the JMX service for the Scheduler.
pa.scheduler.core.jmx.port=5822

# Accounting refresh rate from the database in seconds
pa.scheduler.account.refreshrate=180

# RRD data base with statistic history
pa.scheduler.jmx.rrd.name=scheduler_statistics.rrd

# RRD data base step in seconds
pa.scheduler.jmx.rrd.step=4

# User session time (user is automatically disconnect after this time if no request is made to the scheduler)
# negative number indicates that session is infinite (value specified in second)
pa.scheduler.core.usersessiontime=3600

# Timeout for the start task action. Time during which the scheduling could be waiting (in millis)
# this value relies on the system and network capacity
pa.scheduler.core.starttask.timeout=5000

# Maximum number of threads used for the start task action. This property define the number of blocking resources
# until the scheduling loop will block as well.
# As it is related to the number of nodes, this property also define the number of threads used to terminate taskLauncher
pa.scheduler.core.starttask.threadnumber=5

# Maximum number of threads used to send events to clients. This property defines the number of clients
# than can block at the same time. If this number is reached, every clients won't receive events until
# a thread unlock.
pa.scheduler.core.listener.threadnumber=5

#-------------------------------------------------------
#----------------   JOBS PROPERTIES   ------------------
#-------------------------------------------------------

# Job multiplicative factor. (Task id will be jobId*this_factor+taskId)
pa.scheduler.job.factor=10000

# Remove job delay (in second). (The time between getting back its result and removing it from the scheduler)
# Set this time to 0 if you don't want the job to be remove.
pa.scheduler.core.removejobdelay=0

# Automatic remove job delay (in second). (The time between the termination of the job and removing it from the scheduler)
# Set this time to 0 if you don't want the job to be remove automatically.
pa.scheduler.core.automaticremovejobdelay=0

# Remove job in dataBase when removing it from scheduler.
pa.scheduler.job.removeFromDataBase=false

#-------------------------------------------------------
#---------------   TASKS PROPERTIES   ------------------
#-------------------------------------------------------
# Initial time to wait before the re-execution of a task. (in millisecond)
pa.scheduler.task.initialwaitingtime=1000

# Maximum number of execution for a task in case of failure (node down)
pa.scheduler.task.numberofexecutiononfailure=2

# If false user cannot execute java tasks in nodes and must use either forked java tasks of native tasks
pa.scheduler.task.allowjavatasks=true
# If true script tasks are ran in a forked JVM, if false they are ran in the node's JVM
pa.scheduler.task.scripttasks.fork=true

#-------------------------------------------------------
#-------------   DATASPACES PROPERTIES   ---------------
#-------------------------------------------------------

# Default INPUT space URL. The default INPUT space is used inside each job that does not define an INPUT space.
# Normally, the scheduler will start a FileSystemServer on a default location based on the TEMP directory.
# If the following property is specified, this FileSystemServer will be not be started and instead the provided dataspace
# url will be used
#pa.scheduler.dataspace.defaultinput.url=

# The following property can be used in two ways.
# 1) If a "pa.scheduler.dataspace.defaultinput.url" is provided, the defaultinput.path property
#   tells the scheduler where the actual file system is (provided that he has access to it). If the scheduler does not have
#   access to the file system where this dataspace is located then this property must not be set.
#       - On windows, use double backslash in the path, i.e. c:\\users\\...
#       - you can provide a list of urls separated by spaces , i.e. : http://myserver/myspace file:/path/to/myspace
#       - if one url contain spaces, wrap all urls in the list between deouble quotes :
#               "http://myserver/myspace"  "file:/path/to/my space"
# 2) If a "pa.scheduler.dataspace.defaultinput.url" is not provided, the defaultinput.path property will tell the scheduler
#   to start a FileSystemServer on the provided defaultinput.path instead of its default location

### the default location is TEMP/scheduling/defaultinput
#pa.scheduler.dataspace.defaultinput.localpath=

# Host name from which the localpath is accessible, it must be provided if the localpath property is provided
#pa.scheduler.dataspace.defaultinput.hostname=

# The same for the OUPUT (see above explanations in the INPUT SPACE section)
# (concerning the syntax, see above explanations in the INPUT SPACE section)
#pa.scheduler.dataspace.defaultoutput.url=
### the default location is TEMP/scheduling/defaultoutput
#pa.scheduler.dataspace.defaultoutput.localpath=
#pa.scheduler.dataspace.defaultoutput.hostname=

# The same for the GLOBAL space. The GLOBAL space is shared between each users and each jobs.
# (concerning the syntax, see above explanations in the INPUT SPACE section)
#pa.scheduler.dataspace.defaultglobal.url=
### the default location is TEMP/scheduling/defaultglobal
#pa.scheduler.dataspace.defaultglobal.localpath=
#pa.scheduler.dataspace.defaultglobal.hostname

# The same for the USER spaces. A USER space is a per-user global space. An individual space will be created for each user in subdirectories of the defaultuser.localpath.
# Only one file server will be created (if not provided)
# (concerning the syntax, see above explanations in the INPUT SPACE section)
#pa.scheduler.dataspace.defaultuser.url=
### the default location is TEMP/scheduling/defaultuser
#pa.scheduler.dataspace.defaultuser.localpath=
#pa.scheduler.dataspace.defaultuser.hostname=

#-------------------------------------------------------
#----------------   LOGS PROPERTIES   ------------------
#-------------------------------------------------------
# Logs forwarding method
# Possible methods are :
# Simple socket : org.ow2.proactive.scheduler.common.util.logforwarder.providers.SocketBasedForwardingProvider
# SSHTunneled socket : org.ow2.proactive.scheduler.common.util.logforwarder.providers.SocketWithSSHTunnelBasedForwardingProvider
# ProActive communication : org.ow2.proactive.scheduler.common.util.logforwarder.providers.ProActiveBasedForwardingProvider
#
# set this property to empty string to disable log forwarding alltogether
pa.scheduler.logs.provider=org.ow2.proactive.scheduler.common.util.logforwarder.providers.ProActiveBasedForwardingProvider
# Location of server jobs logs (comment to disable job logging to separate files). Can be an absolute path.
pa.scheduler.job.logs.location=.logs/jobs/

#-------------------------------------------------------
#-----------   AUTHENTICATION PROPERTIES   -------------
#-------------------------------------------------------

# path to the Jaas configuration file which defines what modules are available for internal authentication
pa.scheduler.auth.jaas.path=config/authentication/jaas.config

# path to the private key file which is used to encrypt credentials for authentication
pa.scheduler.auth.privkey.path=config/authentication/keys/priv.key

# path to the public key file which is used to encrypt credentials for authentication
pa.scheduler.auth.pubkey.path=config/authentication/keys/pub.key

# LDAP Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, (if the path is absolute) it is directly interpreted
pa.scheduler.ldap.config.path=config/authentication/ldap.cfg

# LDAP2 Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, (if the path is absolute) it is directly interpreted
pa.scheduler.ldap2.config.path=config/authentication/ldap2.cfg


# Login file name for file authentication method
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, the path is absolute, so the path is directly interpreted
pa.scheduler.core.defaultloginfilename=config/authentication/login.cfg

# Group file name for file authentication method
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, the path is absolute, so the path is directly interpreted
pa.scheduler.core.defaultgroupfilename=config/authentication/group.cfg

#Property that define the method that have to be used for logging users to the Scheduler
#It can be one of the following values :
#	- "SchedulerFileLoginMethod" to use file login and group management
#	- "SchedulerLDAPLoginMethod" to use LDAP login management
#	- "SchedulerLDAP2LoginMethod" to use improved LDAP login management
pa.scheduler.core.authentication.loginMethod=SchedulerFileLoginMethod

#-------------------------------------------------------
#------------------   RM PROPERTIES   ------------------
#-------------------------------------------------------
# Path to the Scheduler credentials file for RM authentication
pa.scheduler.resourcemanager.authentication.credentials=config/authentication/scheduler.cred

# Use single or multiple connection to RM :
# (If true)  the scheduler user will do the requests to rm
# (If false) each Scheduler users have their own connection to RM using their scheduling credentials
pa.scheduler.resourcemanager.authentication.single=true

# Set a timeout for initial connection to the RM connection (in ms)
pa.scheduler.resourcemanager.connection.timeout=120000

#-------------------------------------------------------
#--------------   HIBERNATE PROPERTIES   ---------------
#-------------------------------------------------------
# Hibernate configuration file (relative to home directory)
pa.scheduler.db.hibernate.configuration=config/scheduler/database/hibernate/hibernate.cfg.xml

# Drop database before creating a new one
# If this value is true, the database will be dropped and then re-created
# If this value is false, database will be updated from the existing one.
pa.scheduler.db.hibernate.dropdb=false

# This property is used to limit number of finished jobs loaded from the database
# at scheduler startup. For example setting this property to '10d' means that
# scheduler should load only finished jobs which were submitted during last
# 10 days. In the period expression it is also possible to use symbols 'h' (hours)
# and 'm' (minutes).
# If property isn't set then all finished jobs are loaded.
pa.scheduler.db.load.job.period=

# Set to true to enable email notificaions about finished jobs. Emails
# are sent to the address specified in the generic information of a
# job with the key EMAIL; example:
#    <genericInformation>
#        <info name="EMAIL" value="user@example.com"/>
#    </genericInformation>
pa.scheduler.notifications.email.enabled=false
# From address for notificaions emails (set it to a valid address if
# you would like email notifications to work)
pa.scheduler.notifications.email.from=
----

=== Resources Manager Properties

[source]
----
# definition of all java properties used by resource manager
# warning : definition of these variables can be override by user at JVM startup,
# using for example -Dpa.rm.home=/foo, in the java command

# name of the ProActive Node containing RM's active objects
pa.rm.node.name=RM_NODE

# ping frequency used by node source for keeping a watch on handled nodes (in ms)
pa.rm.node.source.ping.frequency=45000

# ping frequency used by resource manager to ping connected clients (in ms)
pa.rm.client.ping.frequency=45000

# The period of sending "alive" event to resource manager's listeners (in ms)
pa.rm.aliveevent.frequency=300000

# timeout for selection script result
pa.rm.select.script.timeout=30000

# number of selection script digests stored in the cache to predict the execution results
pa.rm.select.script.cache=10000

# The time period when a node has the same dynamic characteristics (in ms).
# It needs to pause the permanent execution of dynamic scripts on nodes.
# Default is 5 mins, which means that if any dynamic selection scripts returns
# false on a node it won't be executed there at least for this time.
pa.rm.select.node.dynamicity=300000

# The full class name of the policy selected nodes
pa.rm.selection.policy=org.ow2.proactive.resourcemanager.selection.policies.ShufflePolicy

# Timeout for remote script execution (in ms)
pa.rm.execute.script.timeout=180000

# If set to non-null value the resource manager executes only scripts from this directory.
# All other selection scripts will be rejected.
pa.rm.select.script.authorized.dir=

# timeout for node lookup
pa.rm.nodelookup.timeout=30000

# GCM application (GCMA) file path, used to perform GCM deployments
# If this file path is relative, the path is evaluated from the Resource manager dir (ie application's root dir)
# defined by the "pa.rm.home" JVM property
# else, the path is absolute, so the path is directly interpreted
pa.rm.gcm.template.application.file=config/rm/deployment/GCMNodeSourceApplication.xml

# java property string defined in the GCMA defined above, which is dynamically replaced
# by a GCM deployment descriptor file path to deploy
pa.rm.gcmd.path.property.name=gcmd.file

# Resource Manager home directory
pa.rm.home=.

# Lists of supported infrastructures in the resource manager
pa.rm.nodesource.infrastructures=config/rm/nodesource/infrastructures

# Lists of supported node acquisition policies in the resource manager
pa.rm.nodesource.policies=config/rm/nodesource/policies

# Max number of threads in node source for parallel task execution
pa.rm.nodesource.maxthreadnumber=50

# Max number of threads in selection manager
pa.rm.selection.maxthreadnumber=50

# Max number of threads in monitoring
pa.rm.monitoring.maxthreadnumber=5

# Number of threads in the node cleaner thread pool
pa.rm.cleaning.maxthreadnumber=5

#Name of the JMX MBean for the RM
pa.rm.jmx.connectorname=JMXRMAgent

#port of the JMX service for the RM.
pa.rm.jmx.port=5822

#Accounting refresh rate from the database in seconds (0 means disabled)
pa.rm.account.refreshrate=180

# RRD data base with statistic history
pa.rm.jmx.rrd.name=rm_statistics.rrd

# RRD data base step in seconds
pa.rm.jmx.rrd.step=4

# path to the Amazon EC2 account credentials properties file,
# mandatory when using the EC2 Infrastructure
pa.rm.ec2.properties=config/rm/deployment/ec2.properties

#-------------------------------------------------------
#---------------   AUTHENTICATION PROPERTIES   ------------------
#-------------------------------------------------------

# path to the Jaas configuration file which defines what modules are available for internal authentication
pa.rm.auth.jaas.path=config/authentication/jaas.config

# path to the private key file which is used to encrypt credentials for authentication
pa.rm.auth.privkey.path=config/authentication/keys/priv.key

# path to the public key file which is used to encrypt credentials for authentication
pa.rm.auth.pubkey.path=config/authentication/keys/pub.key

# LDAP Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, (if the path is absolute) it is directly interpreted
pa.rm.ldap.config.path=config/authentication/ldap.cfg

# LDAP2 Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, (if the path is absolute) it is directly interpreted
pa.rm.ldap2.config.path=config/authentication/ldap2.cfg

# Login file name for file authentication method
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, the path is absolute, so the path is directly interpreted
pa.rm.defaultloginfilename=config/authentication/login.cfg

# Group file name for file authentication method
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, the path is absolute, so the path is directly interpreted
pa.rm.defaultgroupfilename=config/authentication/group.cfg

#Property that define the method that have to be used for logging users to the resource manager
#It can be one of the following values :
#	- "RMFileLoginMethod" to use file login and group management
#	- "RMLDAPLoginMethod" to use LDAP login management
#	- "RMLDAP2LoginMethod" to use improved LDAP login management
pa.rm.authentication.loginMethod=RMFileLoginMethod

# Path to the rm credentials file for authentication
pa.rm.credentials=config/authentication/rm.cred

#-------------------------------------------------------
#--------------   HIBERNATE PROPERTIES   ---------------
#-------------------------------------------------------
# Hibernate configuration file (relative to home directory)
pa.rm.db.hibernate.configuration=config/rm/database/hibernate/hibernate.cfg.xml

# Drop database before creating a new one
# If this value is true, the database will be dropped and then re-created
# If this value is false, database will be updated from the existing one.
pa.rm.db.hibernate.dropdb=false

# Drop only node sources from the data base
pa.rm.db.hibernate.dropdb.nodesources=false

#-------------------------------------------------------
#--------------   TOPOLOGY  PROPERTIES   ---------------
#-------------------------------------------------------
pa.rm.topology.enabled=true

# Pings hosts using standard InetAddress.isReachable() method.
pa.rm.topology.pinger.class=org.ow2.proactive.resourcemanager.frontend.topology.pinging.HostsPinger
# Pings ProActive nodes using Node.getNumberOfActiveObjects().
#pa.rm.topology.pinger.class=org.ow2.proactive.resourcemanager.frontend.topology.pinging.NodesPinger

# Location of selection scripts logs (comment to disable job logging to separate files). Can be an absolute path.
pa.rm.logs.selection.location=.logs/jobs/
----

=== Node Sources

The ProActive Resource Manager supports nodes aggregation from heterogeneous environments. As a node is just a JVM running somewhere, the process of communication to such nodes is unified and defined by ProActive library. The only part which has to be defined is the procedure of nodes deployment which could be quite different depending on infrastructures and their limitations. After installation of the server and node parts it is possible to configure an automatic nodes deployment. Basically, you can say to the resource manager how to launch JVMs with ProActive nodes and when.

==== Node Source Infrastructure

*Infrastructure manager* is responsible for communicating with an infrastructure. When a new node has to be deployed, an infrastructure manager will launch new JVM or just request an already existing nodes running somewhere. All these details are specific to the infrastructure manager implementation.

==== Node Source Policy


*Node source policy* is a set of rules and conditions which describes when and how many nodes have to be acquired or released. Policies use node source API to manage the node acquisition.

Node sources were designed in a way that:

* All logic related to node acquisition is encapsulated in the infrastructure manager.
* Conditions and rules of node acquisition is described in the node source policy.
* Permissions to the node source. Each policy has two parameters:
* nodeUsers - utilization permission defined who can get nodes for computations from this node source. It has to take one of the following values:
** "ME" - Only the node source creator
** "users=user1,user2;groups=group1,group2;tokens=t1,t2" - Only specific users, groups or tokens. I.e. users=user1 - node access is limited to user1; users=user1;groups=group1 - node access is limited to user1 and all users from group group1; users=user1;tokens=t1 - node access is limited to user1 or anyone who specified token t1. If node access is protected by a token, node will not be found by the resource manager (getNodes request) unless the corresponding token is specified.
** "ALL" - Everybody
* nodeProviders - Provider permission defines who can add nodes to this node source. It should take one of the following values:
** "ME" - Only the node source creator
** "users=user1,user2;groups=group1,group2" - Only specific users or groups (for our example user1, user2, group1 and group2). It is possible to specify only groups or only users.
** "ALL" - Everybody
* The user created the node source is the administrator of this node source. It can add and removed nodes to it, remove the node source itself, but cannot use nodes if usage policy is set to PROVIDER or PROVIDER_GROUPS (unless it's granted AllPermissions).
* New infrastructure manager or node source policy can be dynamically plugged into the Resource Manager. In order to do that, it is just required to add new implemented classes in the class path and update corresponding list in the configuration file ([RM_HOME]/config/rm/nodesource).


In the resource manager, there is always a default node source consisted of DefaultInfrastrucureManager and Static policy. It is not able to deploy nodes anywhere but makes it possible to add existing nodes to the RM.

=== CLI tools

include::../dedication.adoc[]

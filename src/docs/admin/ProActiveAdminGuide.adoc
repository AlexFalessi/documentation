= ProActive Administration Guide
include::../common.adoc[]
:imagesdir: src/docs/admin/images

=== Overview

The administration guide covers cluster setup and cluster administration. Cluster setup includes two main steps

* the installation and configuration of the *scheduler server*
* *computing nodes* setup

image::architecture.png[]

=== Getting Started

==== What you will need

* http://www.oracle.com/technetwork/java/javase/downloads/index.html[JDK 1.6] or later
* http://www.activeeon.com/community-downloads[Proactive Scheduler]

NOTE: The Proactive Scheduler works with OpenJDK but it is recommended to use JDK from Oracle.

==== Set up from scratch

http://www.activeeon.com/community-downloads[Download] Proactive Scheduler and *unzip* the archive.

The extracted folder will be referenced as +PROACTIVE_HOME+ in the rest of the documentation
The distribution contains all required dependencies.

==== Running Proactive Scheduler

Proactive Scheduler is ready to be started without an extra configuration.

[source]
----
$ cd PROACTIVE_HOME/bin
----

----
$ jrunscript start-server.js
----

TIP: If the command +jrunscript+ cannot be found, http://docs.oracle.com/javase/7/docs/webnotes/install/windows/jdk-installation-windows.html#path[check that the +PATH+ environment variable contains the JDK executables].

When started, Scheduler, Resource Manager and Web portals URLs are displayed.

[source]
----
---------------------------------
    Starting server processes
---------------------------------

Running Resource Manager process ...
> Starting the resource manager...
> The resource manager with 4 local nodes created on pnp://hostname.local:64738/
Resource Manager stdout/stderr redirected into /home/user/proactive_scheduling/.logs/RM-stdout.log

Running Scheduler process ...
> RM URL : pnp://localhost:64738
> Starting the scheduler...
> Connecting to the resource manager on pnp://localhost:64738
> The scheduler created on pnp://hostname.local:52845/
Scheduler stdout/stderr redirected into /home/user/proactive_scheduling/.logs/Scheduler-stdout.log

Running Jetty process ...
Jetty stdout/stderr redirected into /home/user/proactive_scheduling/.logs/Jetty-stdout.log
Waiting for jetty to start ...
Rest Server webapp deployed at      http://localhost:8080/rest
Resource Manager webapp deployed at http://localhost:8080/rm
Scheduler webapp deployed at        http://localhost:8080/scheduler

Preparing to wait for processes to exit ...
Hit CTRL+C or enter 'exit' to terminate all server processes and exit
----

As presented in the <<_overview>>, all the ProActive Scheduler components are started :

 * *Resource Manager* that handles all nodes on a cluster
 * *Scheduler* that accepts jobs from users and orders them
 * *REST API* for the Resource Manager and Scheduler
 * *Web portals* for the Resource Manager and Scheduler
 * *Workflow Studio* that is the web interface for designing you distributed computations.
 * 4 local *computing nodes* (JVM)

Now your Proactive Scheduler is ready to execute jobs!


=== Configuring the Server


.ProActive configuration files
|===
|Component | Description|File |Reference

.1+^.^|Scheduler
|Scheduling Properties|config/scheduler/settings.ini|<<TODO>>

.1+^.^|Resource Manager
|Node management configuration|config/scheduler/settings.ini|<<TODO>>

.1+^.^|Networking
|Network, firewall, protocols configuration|config/proactive/ProActiveConfiguration.xml|<<TODO>>

.4+^.^|Security
|User logins and passwords|config/authentication/login.cfg|<<TODO>>
|User group assignments|config/authentication/group.cfg|<<TODO>>
|User permissions|config/security.java.policy-server|<<TODO>>
|LDAP configuration|config/authentication/ldap.cfg|<<TODO>>

|===

All configuration files of Proactive Scheduler can be found inside the distribution. First you need tu set up
*users and permissions*

* user and group configs
----
$ config/authentication/login.cfg
$ config/authentication/group.cfg
----
* users permissions
----
$ config/security.java.policy-server
----

To change *server parameters* you may want to look into

* job scheduling configuration
----
$ config/scheduler/settings.ini
----
* node management configuration
----
$ config/rm/settings.ini
----

*Advanced* configuration involves protocols setup, firewall and ldap configuration.

* network, firewall, protocols configuration
----
$ config/proactive/ProActiveConfiguration.xml
----
* ldap user authentication parameters
----
$ config/authentication/ldap.cfg
----


=== Installation on a Cluster

Adding machines of a cluster to the ProActive Scheduler, typically involves unpacking the software on all those host machines.
Once it's done you need to run a computing node daemon on the target machine and connect it to the scheduler. There are two principal ways of
doing that

* by manually launching a process on the remote side and adding it to the server
* by initiating the deployment from the server side - http://tbd[node source creation]

If you are not familiar with Proactive Scheduler you may want to try the *first method* as it's easier to understand.
With the combination of http://tbd[Proactive Agent] it gives you the same result.

The *second method* implies that you have an access to remote hosts (e.g. ssh access) and you want to start/stop computing nodes
by launching commands over one of protocols we support. For instance it can be useful when before launching an computing node you'd
like to deploy a virtual machine.

==== Running a Node

Lets take a closer look at the first method described above. To deploy a node from the remote machine you need

* access this machine (remote desktop, ssh or just go to the lab)
* go to the folder of the Proactive Scheduler
* run the following command
----
$ bin/unix/rm-start-node -r pnp://localhost:64738
----
*-r* options is used to specify the url of the *resource manager*. You can find this url in the output of the server launcher.

It is also possible to launch a node without even copying proactive on a remote machine. To to is you need

* open a browser on a remote host
* go to the http://localhost:8080/rm[resource manager portal]
* use default demo/demo account to access the portal
* choose *Portal->Launch* to download *node.jar*
* launch it
----
$ java -jar node.jar
----

It should connect to the scheduler automatically.

TIP: Launch several processes if you would like to execute several computations at the same time on a node. Each
node process allows to execute one task at the same time.

==== SSH

The second way of deploying nodes is to create *node sources* on the server side.

TIP: A *Node Source* is a set of nodes running on the same infrastructure and having the same access policy.
E.g. a cluster with ssh access where nodes are available from 9 a.m. to 9 p.m. Or another example: nodes
from Amazon EC2 available permanently for users from group 'cloud'.

When node source is defined you choose an *infrastructure manager* from one of supported http://tbd[infrastructures]
 and a http://tbd[policy] that defines rules and limitations of nodes utilization.

To create a node source you can either

* use a web interface of the *resource manager* ('Add Nodes' menu)
* use the http://tbd[rest api]
* use the http://tbd[command line client] inside the distribution

MISSING

==== Agents

In your production environment you might want to control and limit the resources utilization for all hosts in your network,
especially if these are desktop machines where people do their daily activities. Using *Proactive Agent* you can

* control the number of ProActive nodes on each machine
* launch nodes automatically when a machine starts
* restart JVM processes if they fail by some reason and reconnect them to Proactive Scheduler

*Agents* exists for both +Linux+ and +Windows+ operating systems.

===== Windows Agent

The ProActive Windows Agent is a *Windows Service*: a long-running executable that performs specific
functions and which is designed not to require user intervention. The agent is able to create a ProActive
computational resource on the local machine and add them to *Proactive Resource Manager*.

After being installed it

* Loads the user's configuration.
* Creates schedules according to the working plan specified in the configuration.
* Spawns a JVM process that will run a specified java class depending on the selected connection type. 3 types of connections are available:
** *Local Registration* - The specified java class will create a ProActive local node as a computational resource and register it locally.
** *Resource Manager Registration* - The specified java class will create a ProActive local node as a computational resource and register
it in the specified Resource Manager, thus being able to execute java or native tasks received from the Scheduler.It is is important
to note that a JVM process running tasks can potentially spawn child processes.
** *Custom* - The user can specify its own java class.
* Watches the spawned JVM process in order to comply to the following limitations:
** *RAM limitation* - The user can specify a maximum amount of memory allowed for a JVM process and its children. If the limit is reached, then all processes are automatically killed.
** *CPU limitation* - The user can specify a maximum CPU usage allowed for a JVM process and its children. If the limit is exceeded by the sum of CPU usages of all processes, they are automatically throttled to reach the given limit.
* Restarts the spawned JVM process in case of failures with a timeout policy.

===== Install Agents on Windows

* The ProActive Windows Agent installation pack is available on the official http://proactive.inria.fr/[ProActive website]. Follow the links and get the latest version.
* Run the +setup.exe+ file and follow instructions
* When the following dialog appears
+
image::install_config.png[]
** Specify the directory that will contain the *configuration* file named +PAAgent-config.xml+, note that if this file already exists in the specified directory it will be re-used.
** Specify the directory that will contain the *log files* of the ProActive Agent and the spawned runtimes.
** Specify an existing, local *account* under which the ProActive Runtime(s) will be spawned. It is highly recommended to specify an account that is not part of the Administrators group to isolate the ProActive Runtime and reduce security risks.
** The *password* is encrypted using Microsoft AES Cryptographic Provider and only Administrators have access permissions to the keyfile (restrict.dat) this is done using the SubInACLtool.
** If the specified account does not exist the installation program will prompt the user to create a non-admin account with the required privileges.
** Note that the ProActive Agent service is installed under LocalSystem account, this should not be changed, however it can be using the +services.msc+ utility. ("Control Panel->Administrative Tools->Services")
** If you want that any non-admin user (except guest accounts) to be able to start/stop the ProActive Agent service check the "Allow everyone to start/stop" box. If this option is checked the installer will use the SubInACL tool. If the tool is not installed in the +Program Files\Windows Resource Kits\Tools+ directory the installer will try to download its installer from the official Microsoft page.
* The installer will check whether the selected user account has the required privileges. If not follow the steps to add these privileges:
** In the 'Administrative Tools' of the 'Control Panel', open the 'Local Security Policy'.
** In 'Security Settings', select 'Local Policies' then select 'User Rights Assignments'.
** Finally, in the list of policies, open the properties of 'Replace a process-level token' policy and add the needed user. Do the same for 'Adjust memory quotas for a process'. For more information about these privileges refer to the official Microsoft page.

At the end of the installation, the ProActive Agent Control utility should be started. This next section explains how to configure it.

To uninstall the ProActive Windows Agent, simply run "Start->Programs->ProActiveAgent->Uninstall ProActive Agent".

===== Configure Agents on Windows

Launch "Start->Programs->ProActiveAgent->AgentControl" program or click on the notify icon if the "Automatic launch" is activated.
Double click on the tray icon to open the *ProActive Agent Control* window. The following window will appear:

image::agent_control.png[]

From the ProActive Agent Control window, the user can load a configuration file, edit it, start/stop the service and view logs.
A GUI for editing is provided (explained below). Even if it is not recommended, the user can edit the configuration file by yourself with your favorite text editor.

It is also possible to change the ProActive Runtime Account using the "Change Account" button.

Clicking on "GUI Edit", the following windows appears:

image::config_editor_general.png[]


In the general tab, the user can specify:

* The *ProActive Scheduler* location.
* The *JRE* location (usually something like +C:\Program Files\Java\jdk1.6.0_12+).
* The *number* of Runtimes (the number of spawned JVMs).
* The *JVM options*.
* Note that if the parameter contains +${rank}+, it will be dynamically replaced by the Runtime rank starting from 0.
* The "On Runtime Exit" script. A script executed after a Runtime exits. This can be useful to perform additional cleaning operation.
* Note that the script receives as parameter the PID of the Runtime.
* The user can set a *memory limit* that will prevent the spawned processes to exceed a specified amount of RAM. If a spawned process or its child process requires more memory, it will be killed as well as its child processes.
* Note that this limit is disabled by default (0 means no limit) and a ProActive Runtime will require at least 128 MBytes.
* It is possible to list all available *network interfaces* by clicking on the "Refresh" button and add the selected network interface name as a value of the +proactive.net.interface+ property by clicking on "Use" button. See the ProActive documentation for further information.
* The user can specify the *protocol* (rmi or http) to be used by the Runtime for incoming communications.
* To ensure that a unique port is used by a Runtime, the initial port value will be incremented for each node process and given as value of the +-Dproactive.SELECTED_PROTOCOL.port+ JVM property. If the port chosen for a runtime is already used, it is incremented until an available port number is found.

Clicking on the "Connection" tab, the windows will look like this:

image::config_editor_connection.png[]

In the "Connection" tab, the user can select between three types of connections:

* *Local Registration* - creates a local ProActive node and registers (advertises) it in a local RMI registry. The node name is optional.
* *Resource Manager Registration* - creates a local ProActive node and registers it in the specified Resource Manager. The mandatory Resource Manager's url must be like 'protocol://host:port/'. The node name and the node source name are optional. Since the Resource Manager requires an authentication, the user specifies the file that contains the credential. If no file is specified the default one	usually located in +%USERPROFILE%\.proactive\security+ folder is used.
* *Custom* - the user specifies its own java starter class and the arguments to be given to the main method. The java starter class must be in the classpath when the JVM process is started.

Finally, clicking on the "Planning" tab, the windows will look like this:

image::config_editor_planning.png[]

In the Planning Tab, depending on the selected connection type, the agent will initiate it according to a *weekly planning* where each plan specifies the connection start time as well as the working duration. The agent will end the connection as well as the ProActive Runtime process and its child processes when the plan duration has expired.

Moreover, it is possible to specify the JVM process Priority and it's *CPU usage* limit. The behavior of the CPU usage limit works as follows: if the JVM process spawns other processes, they will also be part of the limit so that if the sum of CPU% of all processes exceeds the user limit they will be throttled to reach the given limit. Note that if the Priority is set to RealTime the CPU % throttling will be disabled.

The "Always available" makes the agent to run permanently with a Normal Priority and Max CPU usage at 100%.

===== Launching Windows Agent

Once you have configured the agent, you can start it clicking on the "Start" button of the ProActive Agent Control window. However, before that, you have to ensure that *Proactive Scheduler* has been started on the address you specified in the agent configuration. You do not need to start a node since it is exactly the job of the agent.

Once started, you may face some problems. You can realise that an error occurred by first glancing at the color of the agent tray icon. If everything goes right, it should keep the blue color. If its color changes to yellow, it means that the agent has been stopped. To see exactly what happened, you can look at the runtime log file located into the agent installation directory and named +Executor<runtime number>Process-log.txt+.

The main troubles you may have to face are the following ones:

* You get an *access denied error*: this is probably due to your default java.security.policy file which cannot be found. If you want to specify another policy file, you have to add a JVM parameter in the agent configuration. A policy file is supplied in the scheduling directory. To use it, add the following line in the JVM parameter box of the agent configuration (Figure 5.3, “Configuration Editor window - General Tab ”):
----
-Djava.security.poly=<scheduler directory>/config/security.java.policy-client
----
* You get an *authentication error*: this is probably due to your default credentials file which cannot be found. In the "Connection" tab of the Configuration Editor (Figure 5.4, “Configuration Editor window - Connection Tab (Resource Manager Registration)”), you can choose the credentials file you want. You can select, for instance, the credentials file located at <scheduler directory>/config/authentication/scheduler.cred or your own credentials file.
* The node seems to be well started but you cannot see it in the Resource Manager interface : in this case, make sure that the *port number* is the good one. Do not forget that the runtime port number is incremented from the initial resource manager port number. You can see exactly on which port your runtime has been started looking at the log file describing above.

===== Install Agents on Linux

MISSING

=== Installation on a Cluster with Firewall

MISSING

=== Control the resource usage
=== Policies
MISSING
=== Agents schedule
MISSING


=== User Authentication

In order to use Proactive Scheduler every user must have an account. We support two methods of authentication

* *file based*
* *ldap*

==== Select authentication method

By default Proactive Scheduler is configured to use *file based* authentication and has some default accounts ('demo/demo', 'admin/admin') that
works out of the box.

If you'd like to use your *LDAP* server you need to modify two configs

* *resource manager* configuration
+
[source]
----
config/rm/settings.ini

#Property that defines the method that has to be used for logging users to the Resource Manager
#It can be one of the following values:
#    - "RMFileLoginMethod" to use file login and group management
#    - "RMLDAPLoginMethod" to use LDAP login management
pa.rm.authentication.loginMethod=RMLDAPLoginMethod
----

* *scheduler* configuration
+
[source]
----
config/scheduler/settings.ini

#Property that define the method that have to be used for logging users to the Scheduler
#It can be one of the following values :
#	- "SchedulerFileLoginMethod" to use file login and group management
#	- "SchedulerLDAPLoginMethod" to use LDAP login management
#	- "SchedulerLDAP2LoginMethod" to use improved LDAP login management
pa.scheduler.core.authentication.loginMethod=SchedulerLDAPLoginMethod
----


==== File

By default, the resource manager stores users accounts, passwords, and group memberships (user or admin), in two files

* users and passwords accounts are stored in +config/authentication/login.cfg+. Each line has to look like *user:passwd*. The default +login.cfg+ file is given hereafter:
[source]
----
admin:admin
user:pwd
demo:demo
----

* users membership is stored in +config/authentication/group.cfg+. For each user registered in login.cfg, a group membership has to be defined in this file. Each line has to look like user:group. Group has to be user to have user rights, or admin to have administrator rights. Default group.cfg is like this:

[source]
----
admin:admin
demo:admin
user:user
----

==== LDAP

The resource manager is able to connect to an existing *LDAP*, to check users login/password and verify users group
membership. This authentication method can be used with existing LDAP server which is already configured.
In order to use it, few parameters have to be configured, such as *path in LDAP tree users*, LDAP *groups* that define
user and admin group membership, *URL* of the LDAP server, LDAP *binding method* used by connection and configuration
of SSL/TLS if you want a secured connection between the resource manager and LDAP.

We assume that LDAP server is configured in the way that:

* all existing users and groups are located under single domain
* users have object class specified in parameter *pa.ldap.user.objectclass*
* groups have object class specified in parameter *pa.ldap.group.objectclass*
* user and group name is defined in cn (Common Name) attribute

[source]
----
# EXAMPLE of user entry
#
# dn: cn=jdoe,dc=example,dc=com
# cn: jdoe
# firstName: John
# lastName: Doe
# objectClass: inetOrgPerson

# EXAMPLE of group entry
#
# dn: cn=mygroup,dc=example,dc=com
# cn: mygroup
# firstName: John
# lastName: Doe
# uniqueMember: cn=djoe,dc=example,dc=com
# objectClass: groupOfUniqueNames
----

Default value for this property defines a default configuration file +config/authentication/ldap.cfg+. Specify your LDAP properties in this file. Properties are explained below.

. *Set LDAP url*
+
First, you have to define the LDAP's URL of your organisation. This address corresponds to the property: pa.ldap.url. You have to put a standard LDAP-like URL, for example *ldap://myLdap*. You can also set an URL with secure access: *ldaps://myLdap:636*.
+
. *Define object class of user and group entities*
+
Then you need to define how to differ user and group entities in LDAP tree. The users object class is defined by property *pa.ldap.user.objectclass* and by default is "inetOrgPerson". For groups, the property pa.ldap.group.objectclass has a default value +groupOfUniqueNames+ which could be changed.

. *Configure LDAP authentication parameters*
+
By default, the Proactive Scheduler binds to LDAP in anonymous mode. You can change this authentication method by modifying the property *pa.ldap.authentication.method*. This property can have several values:
+
* none (default value) - the resource manager performs connection to LDAP in anonymous mode.
* simple - the resource manager performs connection to LDAP with a specified login/password (see below for user password setting).
+
You can also specify a SASL mechanism for LDAPv3. There are many SASL available mechanisms: cram-md5, digest-md5, kerberos4. Just put sasl to this property to let the resource manager JVM choose SASL authentication mechanism.
If you specify an authentication method different from 'none' (anonymous connection to LDAP), you must specify a login/password for authentication. There are two properties to set in LDAP configuration file:
+
* *pa.ldap.bind.login* - sets user name for authentication.
* *pa.ldap.bind.pwd* - sets password for authentication.
+
. *Set SSL/TLS parameters*
+
A secured SSL/TLS layer can be useful if your network is not trusted, and critical information are transmitted between the rm server and LDAP, such as user passwords.
First, set the LDAP URL property *pa.ldap.url* to a URL of type *ldaps://myLdap*. Then set *pa.ldap.authentication.method* to none so as to delegate authentication to SSL.
+
For using SSL properly, you have to specify your certificate and public keys for SSL handshake. Java stores certificates in a keyStore and public keys in a trustStore. In most of the cases, you just have to define a trustStore with public key part of LDAP's certificate. Put certificate in a keyStore, and public keys in a trustStore with the keytool command (keytool command is distributed with standard java platforms):
+

    keytool -import -alias myAlias -file myCertificate -keystore myKeyStore
+
myAlias is the alias name of your certificate, myCertificate is your private certificate file and myKeyStore is the new keyStore file produced in output. This command asks you to enter a password for your keyStore.
+
Put LDAP certificate's public key in a trustStore, with the keytool command:
+
    keytool -import -alias myAlias -file myPublicKey -keystore myTrustStore
+
myAlias is the alias name of your certificate's public key, myPublicKey is your certificate's public key file and myTrustore is the new trustStore file produced in output. This command asks you to enter a password for your trustStore.
+
Finally, in +config/authentication/ldap.cfg+, set keyStore and trustStore created before to their respective passwords:
+
* Set *pa.ldap.keystore.path* to the path of your keyStore.
* Set *pa.ldap.keystore.passwd* to the password defined previously for keyStore.
* Set *pa.ldap.truststore.path* to the path of your trustStore.
* Set *pa.ldap.truststore.passwd* to the password defined previously for trustStore.

. *Use fall back to file authentication*
+
You can use simultaneously file-based authentication and LDAP-based authentication. Then Proactive Scheduler can check user password and group membership in
login and group files, as performed in FileLogin method, if user or group is not found in LDAP.
It uses *pa.rm.defaultloginfilename* and *pa.rm.defaultgroupfilename* files to authenticate user and check group membership. There are two rules:

* If LDAP group membership checking fails, fall back to group membership checking with group file.
To activate this behavior set *pa.ldap.group.membership.fallback* to true, in LDAP configuration file.
* If a user is not found in LDAP, fall back to authentication and group membership checking with login
and group files. To activate this behavior, set *pa.ldap.authentication.fallback* to true, in LDAP configuration file.

=== User Permissions

All users authenticated in the resource manager have they own role according to granted permissions.
In Proactive Scheduler, we use standard *Java Authentication and Authorization Service* (JAAS) to address these needs.

----
config/security.java.policy-server
----

Il allows to configure fine-grained access for all users, e.g. how has a right to

* deploy nodes
* execute jobs
* pause the scheduler
* etc

Users are organized in groups and after authentication each of them has a single *UserPrincipal* and some *GroupPrincipals*.
Using this principals, the permissions are granted in the security policy file, like the following:

[source]
----
grant principal org.ow2.proactive.authentication.principals.UserNamePrincipal "john" {
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.resourcemanager.core.RMCore.getAtMostNodes";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.resourcemanager.core.RMCore.releaseNodes";
};
----

This means that user "john" can request nodes for computations and release them. It cannot perform any administrative
actions (they have to be listed explicitly).

Permissions could be granted to groups and in this case will be applicable to all group members. For example, we may
define a group of users who has right to submit jobs to the scheduler and change job priorities from level 1 to 3


[source]
----
grant principal org.ow2.proactive.authentication.principals.GroupNamePrincipal "users" {
    permission org.ow2.proactive.scheduler.permissions.GetOwnStateOnlyPermission "true";
    permission org.ow2.proactive.scheduler.permissions.ChangePriorityPermission "1,2,3";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.submit";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.getJobResult";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.getTaskResult";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.getTaskResultFromIncarnation";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.killTask";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.restartTask";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.getJobState";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.removeJob";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.listenJobLogs";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.getStatus";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.getState";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.addEventListener";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.pauseJob";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.resumeJob";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.killJob";
    permission org.ow2.proactive.permissions.MethodCallPermission
            "org.ow2.proactive.scheduler.core.SchedulerFrontend.changeJobPriority";

};
----

There is a permission which authorized to perform any action
*org.ow2.proactive.permissions.AllPermission*
The following is the usage of this permission in group of administrators:

[source]
----
grant principal org.ow2.proactive.authentication.principals.GroupNamePrincipal "admin" {
    permission org.ow2.proactive.permissions.AllPermission;
};
----


=== Monitor the cluster state
==== Nodes States

In order to provide an access to shared resources, the resource manager maintains different node states:

* Deploying - The deployment of the node has been triggered by the resource manager but it has not yet been added.
* Lost - The deployment of the node has failed for some reason. The node has never been added to the resource manager and won't be usable.
* Configuring - Node has been added to the resource manager and is under configuration. The resource manager computes several information about the node. This step can be time consuming depending.
* Free - Node is available for computations.
* Busy - Node has been given to user to execute computations.
* Locked - Node is under maintenance and cannot be used for computations.
* To be removed - Node is busy but requested to be removed. So it will be removed once the client will release it.
* Down - Node is unreachable or down and cannot be used anymore.

MISSING

==== JMX

MISSING

==== Accounting

MISSING

=== Troubleshooting
==== Logs

MISSING

=== References

==== Scheduler Properties

[source]
----
# Scheduler home directory (this default value should be proper in most cases)
pa.scheduler.home=.

# Timeout for the scheduling loop (in millisecond)
pa.scheduler.core.timeout=2000

# Auto-reconnection to the Resource Manger
pa.scheduler.core.rmconnection.autoconnect = true
pa.scheduler.core.rmconnection.timespan = 5000
pa.scheduler.core.rmconnection.attempts = 20

# Number of threads used to execute client requests
pa.scheduler.core.clientpoolnbthreads=5

# Number of threads used to execute internal scheduling operations
pa.scheduler.core.internalpoolnbthreads=5

# Check for failed node frequency (in second)
pa.scheduler.core.nodepingfrequency=20

# Cache classes definition in task class servers
pa.scheduler.classserver.usecache=true;

# Temporary directory for jobclasspathes
# pa.scheduler.classserver.tmpdir=TO/BE/SET;

# Scheduler default policy full name
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.DefaultPolicy

# Defines the maximum number of tasks to be scheduled in each scheduling loop.
pa.scheduler.policy.nbtaskperloop=10

# Forked java task default security policy path (use to define the policy of the forked task)
pa.scheduler.forkedtask.security.policy=config/scheduler/forkedJavaTask/forkedTask.java.policy

# Log4J forked java task default file path
pa.scheduler.forkedtask.log4j=config/scheduler/forkedJavaTask/forkedTask-log4j

# ProActiveConfiguration forked java task default file path
pa.scheduler.forkedtask.paconfig=config/scheduler/forkedJavaTask/forkedTask-paconf.xml

#Name of the JMX MBean for the scheduler
pa.scheduler.core.jmx.connectorname=JMXSchedulerAgent

# port of the JMX service for the Scheduler.
pa.scheduler.core.jmx.port=5822

# Accounting refresh rate from the database in seconds
pa.scheduler.account.refreshrate=180

# RRD data base with statistic history
pa.scheduler.jmx.rrd.name=scheduler_statistics.rrd

# RRD data base step in seconds
pa.scheduler.jmx.rrd.step=4

# User session time (user is automatically disconnect after this time if no request is made to the scheduler)
# negative number indicates that session is infinite (value specified in second)
pa.scheduler.core.usersessiontime=3600

# Timeout for the start task action. Time during which the scheduling could be waiting (in millis)
# this value relies on the system and network capacity
pa.scheduler.core.starttask.timeout=5000

# Maximum number of threads used for the start task action. This property define the number of blocking resources
# until the scheduling loop will block as well.
# As it is related to the number of nodes, this property also define the number of threads used to terminate taskLauncher
pa.scheduler.core.starttask.threadnumber=5

# Maximum number of threads used to send events to clients. This property defines the number of clients
# than can block at the same time. If this number is reached, every clients won't receive events until
# a thread unlock.
pa.scheduler.core.listener.threadnumber=5

#-------------------------------------------------------
#----------------   JOBS PROPERTIES   ------------------
#-------------------------------------------------------

# Job multiplicative factor. (Task id will be jobId*this_factor+taskId)
pa.scheduler.job.factor=10000

# Remove job delay (in second). (The time between getting back its result and removing it from the scheduler)
# Set this time to 0 if you don't want the job to be remove.
pa.scheduler.core.removejobdelay=0

# Automatic remove job delay (in second). (The time between the termination of the job and removing it from the scheduler)
# Set this time to 0 if you don't want the job to be remove automatically.
pa.scheduler.core.automaticremovejobdelay=0

# Remove job in dataBase when removing it from scheduler.
pa.scheduler.job.removeFromDataBase=false

#-------------------------------------------------------
#---------------   TASKS PROPERTIES   ------------------
#-------------------------------------------------------
# Initial time to wait before the re-execution of a task. (in millisecond)
pa.scheduler.task.initialwaitingtime=1000

# Maximum number of execution for a task in case of failure (node down)
pa.scheduler.task.numberofexecutiononfailure=2

# If false user cannot execute java tasks in nodes and must use either forked java tasks of native tasks
pa.scheduler.task.allowjavatasks=true
# If true script tasks are ran in a forked JVM, if false they are ran in the node's JVM
pa.scheduler.task.scripttasks.fork=true

#-------------------------------------------------------
#-------------   DATASPACES PROPERTIES   ---------------
#-------------------------------------------------------

# Default INPUT space URL. The default INPUT space is used inside each job that does not define an INPUT space.
# Normally, the scheduler will start a FileSystemServer on a default location based on the TEMP directory.
# If the following property is specified, this FileSystemServer will be not be started and instead the provided dataspace
# url will be used
#pa.scheduler.dataspace.defaultinput.url=

# The following property can be used in two ways.
# 1) If a "pa.scheduler.dataspace.defaultinput.url" is provided, the defaultinput.path property
#   tells the scheduler where the actual file system is (provided that he has access to it). If the scheduler does not have
#   access to the file system where this dataspace is located then this property must not be set.
#       - On windows, use double backslash in the path, i.e. c:\\users\\...
#       - you can provide a list of urls separated by spaces , i.e. : http://myserver/myspace file:/path/to/myspace
#       - if one url contain spaces, wrap all urls in the list between deouble quotes :
#               "http://myserver/myspace"  "file:/path/to/my space"
# 2) If a "pa.scheduler.dataspace.defaultinput.url" is not provided, the defaultinput.path property will tell the scheduler
#   to start a FileSystemServer on the provided defaultinput.path instead of its default location

### the default location is TEMP/scheduling/defaultinput
#pa.scheduler.dataspace.defaultinput.localpath=

# Host name from which the localpath is accessible, it must be provided if the localpath property is provided
#pa.scheduler.dataspace.defaultinput.hostname=

# The same for the OUPUT (see above explanations in the INPUT SPACE section)
# (concerning the syntax, see above explanations in the INPUT SPACE section)
#pa.scheduler.dataspace.defaultoutput.url=
### the default location is TEMP/scheduling/defaultoutput
#pa.scheduler.dataspace.defaultoutput.localpath=
#pa.scheduler.dataspace.defaultoutput.hostname=

# The same for the GLOBAL space. The GLOBAL space is shared between each users and each jobs.
# (concerning the syntax, see above explanations in the INPUT SPACE section)
#pa.scheduler.dataspace.defaultglobal.url=
### the default location is TEMP/scheduling/defaultglobal
#pa.scheduler.dataspace.defaultglobal.localpath=
#pa.scheduler.dataspace.defaultglobal.hostname

# The same for the USER spaces. A USER space is a per-user global space. An individual space will be created for each user in subdirectories of the defaultuser.localpath.
# Only one file server will be created (if not provided)
# (concerning the syntax, see above explanations in the INPUT SPACE section)
#pa.scheduler.dataspace.defaultuser.url=
### the default location is TEMP/scheduling/defaultuser
#pa.scheduler.dataspace.defaultuser.localpath=
#pa.scheduler.dataspace.defaultuser.hostname=

#-------------------------------------------------------
#----------------   LOGS PROPERTIES   ------------------
#-------------------------------------------------------
# Logs forwarding method
# Possible methods are :
# Simple socket : org.ow2.proactive.scheduler.common.util.logforwarder.providers.SocketBasedForwardingProvider
# SSHTunneled socket : org.ow2.proactive.scheduler.common.util.logforwarder.providers.SocketWithSSHTunnelBasedForwardingProvider
# ProActive communication : org.ow2.proactive.scheduler.common.util.logforwarder.providers.ProActiveBasedForwardingProvider
#
# set this property to empty string to disable log forwarding alltogether
pa.scheduler.logs.provider=org.ow2.proactive.scheduler.common.util.logforwarder.providers.ProActiveBasedForwardingProvider
# Location of server jobs logs (comment to disable job logging to separate files). Can be an absolute path.
pa.scheduler.job.logs.location=.logs/jobs/

#-------------------------------------------------------
#-----------   AUTHENTICATION PROPERTIES   -------------
#-------------------------------------------------------

# path to the Jaas configuration file which defines what modules are available for internal authentication
pa.scheduler.auth.jaas.path=config/authentication/jaas.config

# path to the private key file which is used to encrypt credentials for authentication
pa.scheduler.auth.privkey.path=config/authentication/keys/priv.key

# path to the public key file which is used to encrypt credentials for authentication
pa.scheduler.auth.pubkey.path=config/authentication/keys/pub.key

# LDAP Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, (if the path is absolute) it is directly interpreted
pa.scheduler.ldap.config.path=config/authentication/ldap.cfg

# LDAP2 Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, (if the path is absolute) it is directly interpreted
pa.scheduler.ldap2.config.path=config/authentication/ldap2.cfg


# Login file name for file authentication method
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, the path is absolute, so the path is directly interpreted
pa.scheduler.core.defaultloginfilename=config/authentication/login.cfg

# Group file name for file authentication method
# If this file path is relative, the path is evaluated from the Scheduler dir (ie application's root dir)
# with the variable defined below : pa.scheduler.home.
# else, the path is absolute, so the path is directly interpreted
pa.scheduler.core.defaultgroupfilename=config/authentication/group.cfg

#Property that define the method that have to be used for logging users to the Scheduler
#It can be one of the following values :
#	- "SchedulerFileLoginMethod" to use file login and group management
#	- "SchedulerLDAPLoginMethod" to use LDAP login management
#	- "SchedulerLDAP2LoginMethod" to use improved LDAP login management
pa.scheduler.core.authentication.loginMethod=SchedulerFileLoginMethod

#-------------------------------------------------------
#------------------   RM PROPERTIES   ------------------
#-------------------------------------------------------
# Path to the Scheduler credentials file for RM authentication
pa.scheduler.resourcemanager.authentication.credentials=config/authentication/scheduler.cred

# Use single or multiple connection to RM :
# (If true)  the scheduler user will do the requests to rm
# (If false) each Scheduler users have their own connection to RM using their scheduling credentials
pa.scheduler.resourcemanager.authentication.single=true

# Set a timeout for initial connection to the RM connection (in ms)
pa.scheduler.resourcemanager.connection.timeout=120000

#-------------------------------------------------------
#--------------   HIBERNATE PROPERTIES   ---------------
#-------------------------------------------------------
# Hibernate configuration file (relative to home directory)
pa.scheduler.db.hibernate.configuration=config/scheduler/database/hibernate/hibernate.cfg.xml

# Drop database before creating a new one
# If this value is true, the database will be dropped and then re-created
# If this value is false, database will be updated from the existing one.
pa.scheduler.db.hibernate.dropdb=false

# This property is used to limit number of finished jobs loaded from the database
# at scheduler startup. For example setting this property to '10d' means that
# scheduler should load only finished jobs which were submitted during last
# 10 days. In the period expression it is also possible to use symbols 'h' (hours)
# and 'm' (minutes).
# If property isn't set then all finished jobs are loaded.
pa.scheduler.db.load.job.period=

# Set to true to enable email notificaions about finished jobs. Emails
# are sent to the address specified in the generic information of a
# job with the key EMAIL; example:
#    <genericInformation>
#        <info name="EMAIL" value="user@example.com"/>
#    </genericInformation>
pa.scheduler.notifications.email.enabled=false
# From address for notificaions emails (set it to a valid address if
# you would like email notifications to work)
pa.scheduler.notifications.email.from=
----

==== Resources Manager Properties

[source]
----
# definition of all java properties used by resource manager
# warning : definition of these variables can be override by user at JVM startup,
# using for example -Dpa.rm.home=/foo, in the java command

# name of the ProActive Node containing RM's active objects
pa.rm.node.name=RM_NODE

# ping frequency used by node source for keeping a watch on handled nodes (in ms)
pa.rm.node.source.ping.frequency=45000

# ping frequency used by resource manager to ping connected clients (in ms)
pa.rm.client.ping.frequency=45000

# The period of sending "alive" event to resource manager's listeners (in ms)
pa.rm.aliveevent.frequency=300000

# timeout for selection script result
pa.rm.select.script.timeout=30000

# number of selection script digests stored in the cache to predict the execution results
pa.rm.select.script.cache=10000

# The time period when a node has the same dynamic characteristics (in ms).
# It needs to pause the permanent execution of dynamic scripts on nodes.
# Default is 5 mins, which means that if any dynamic selection scripts returns
# false on a node it won't be executed there at least for this time.
pa.rm.select.node.dynamicity=300000

# The full class name of the policy selected nodes
pa.rm.selection.policy=org.ow2.proactive.resourcemanager.selection.policies.ShufflePolicy

# Timeout for remote script execution (in ms)
pa.rm.execute.script.timeout=180000

# If set to non-null value the resource manager executes only scripts from this directory.
# All other selection scripts will be rejected.
pa.rm.select.script.authorized.dir=

# timeout for node lookup
pa.rm.nodelookup.timeout=30000

# GCM application (GCMA) file path, used to perform GCM deployments
# If this file path is relative, the path is evaluated from the Resource manager dir (ie application's root dir)
# defined by the "pa.rm.home" JVM property
# else, the path is absolute, so the path is directly interpreted
pa.rm.gcm.template.application.file=config/rm/deployment/GCMNodeSourceApplication.xml

# java property string defined in the GCMA defined above, which is dynamically replaced
# by a GCM deployment descriptor file path to deploy
pa.rm.gcmd.path.property.name=gcmd.file

# Resource Manager home directory
pa.rm.home=.

# Lists of supported infrastructures in the resource manager
pa.rm.nodesource.infrastructures=config/rm/nodesource/infrastructures

# Lists of supported node acquisition policies in the resource manager
pa.rm.nodesource.policies=config/rm/nodesource/policies

# Max number of threads in node source for parallel task execution
pa.rm.nodesource.maxthreadnumber=50

# Max number of threads in selection manager
pa.rm.selection.maxthreadnumber=50

# Max number of threads in monitoring
pa.rm.monitoring.maxthreadnumber=5

# Number of threads in the node cleaner thread pool
pa.rm.cleaning.maxthreadnumber=5

#Name of the JMX MBean for the RM
pa.rm.jmx.connectorname=JMXRMAgent

#port of the JMX service for the RM.
pa.rm.jmx.port=5822

#Accounting refresh rate from the database in seconds (0 means disabled)
pa.rm.account.refreshrate=180

# RRD data base with statistic history
pa.rm.jmx.rrd.name=rm_statistics.rrd

# RRD data base step in seconds
pa.rm.jmx.rrd.step=4

# path to the Amazon EC2 account credentials properties file,
# mandatory when using the EC2 Infrastructure
pa.rm.ec2.properties=config/rm/deployment/ec2.properties

#-------------------------------------------------------
#---------------   AUTHENTICATION PROPERTIES   ------------------
#-------------------------------------------------------

# path to the Jaas configuration file which defines what modules are available for internal authentication
pa.rm.auth.jaas.path=config/authentication/jaas.config

# path to the private key file which is used to encrypt credentials for authentication
pa.rm.auth.privkey.path=config/authentication/keys/priv.key

# path to the public key file which is used to encrypt credentials for authentication
pa.rm.auth.pubkey.path=config/authentication/keys/pub.key

# LDAP Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, (if the path is absolute) it is directly interpreted
pa.rm.ldap.config.path=config/authentication/ldap.cfg

# LDAP2 Authentication configuration file path, used to set LDAP configuration properties
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, (if the path is absolute) it is directly interpreted
pa.rm.ldap2.config.path=config/authentication/ldap2.cfg

# Login file name for file authentication method
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, the path is absolute, so the path is directly interpreted
pa.rm.defaultloginfilename=config/authentication/login.cfg

# Group file name for file authentication method
# If this file path is relative, the path is evaluated from the resource manager dir (ie application's root dir)
# with the variable defined below : pa.rm.home.
# else, the path is absolute, so the path is directly interpreted
pa.rm.defaultgroupfilename=config/authentication/group.cfg

#Property that define the method that have to be used for logging users to the resource manager
#It can be one of the following values :
#	- "RMFileLoginMethod" to use file login and group management
#	- "RMLDAPLoginMethod" to use LDAP login management
#	- "RMLDAP2LoginMethod" to use improved LDAP login management
pa.rm.authentication.loginMethod=RMFileLoginMethod

# Path to the rm credentials file for authentication
pa.rm.credentials=config/authentication/rm.cred

#-------------------------------------------------------
#--------------   HIBERNATE PROPERTIES   ---------------
#-------------------------------------------------------
# Hibernate configuration file (relative to home directory)
pa.rm.db.hibernate.configuration=config/rm/database/hibernate/hibernate.cfg.xml

# Drop database before creating a new one
# If this value is true, the database will be dropped and then re-created
# If this value is false, database will be updated from the existing one.
pa.rm.db.hibernate.dropdb=false

# Drop only node sources from the data base
pa.rm.db.hibernate.dropdb.nodesources=false

#-------------------------------------------------------
#--------------   TOPOLOGY  PROPERTIES   ---------------
#-------------------------------------------------------
pa.rm.topology.enabled=true

# Pings hosts using standard InetAddress.isReachable() method.
pa.rm.topology.pinger.class=org.ow2.proactive.resourcemanager.frontend.topology.pinging.HostsPinger
# Pings ProActive nodes using Node.getNumberOfActiveObjects().
#pa.rm.topology.pinger.class=org.ow2.proactive.resourcemanager.frontend.topology.pinging.NodesPinger

# Location of selection scripts logs (comment to disable job logging to separate files). Can be an absolute path.
pa.rm.logs.selection.location=.logs/jobs/
----

==== Node Sources

The ProActive Resource Manager supports nodes aggregation from heterogeneous environments. As a node is just a JVM running somewhere, the process of communication to such nodes is unified and defined by ProActive library. The only part which has to be defined is the procedure of nodes deployment which could be quite different depending on infrastructures and their limitations. After installation of the server and node parts it is possible to configure an automatic nodes deployment. Basically, you can say to the resource manager how to launch JVMs with ProActive nodes and when.

===== Node Source Infrastructure

*Infrastructure manager* is responsible for communicating with an infrastructure. When a new node has to be deployed, an infrastructure manager will launch new JVM or just request an already existing nodes running somewhere. All these details are specific to the infrastructure manager implementation.

===== Node Source Policy


*Node source policy* is a set of rules and conditions which describes when and how many nodes have to be acquired or released. Policies use node source API to manage the node acquisition.

Node sources were designed in a way that:

* All logic related to node acquisition is encapsulated in the infrastructure manager.
* Conditions and rules of node acquisition is described in the node source policy.
* Permissions to the node source. Each policy has two parameters:
* nodeUsers - utilization permission defined who can get nodes for computations from this node source. It has to take one of the following values:
** "ME" - Only the node source creator
** "users=user1,user2;groups=group1,group2;tokens=t1,t2" - Only specific users, groups or tokens. I.e. users=user1 - node access is limited to user1; users=user1;groups=group1 - node access is limited to user1 and all users from group group1; users=user1;tokens=t1 - node access is limited to user1 or anyone who specified token t1. If node access is protected by a token, node will not be found by the resource manager (getNodes request) unless the corresponding token is specified.
** "ALL" - Everybody
* nodeProviders - Provider permission defines who can add nodes to this node source. It should take one of the following values:
** "ME" - Only the node source creator
** "users=user1,user2;groups=group1,group2" - Only specific users or groups (for our example user1, user2, group1 and group2). It is possible to specify only groups or only users.
** "ALL" - Everybody
* The user created the node source is the administrator of this node source. It can add and removed nodes to it, remove the node source itself, but cannot use nodes if usage policy is set to PROVIDER or PROVIDER_GROUPS (unless it's granted AllPermissions).
* New infrastructure manager or node source policy can be dynamically plugged into the Resource Manager. In order to do that, it is just required to add new implemented classes in the class path and update corresponding list in the configuration file ([RM_HOME]/config/rm/nodesource).


In the resource manager, there is always a default node source consisted of DefaultInfrastrucureManager and Static policy. It is not able to deploy nodes anywhere but makes it possible to add existing nodes to the RM.

==== CLI tools

[appendix]
=== Properties reference

[colophon]
[discrete]
=== Legal notice

The Activeeon team (C) 2014 by Activeeon

****
This library is free software; you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation; version 3 of the License.
This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.
You should have received a copy of the GNU Affero General Public License along with this library; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
If needed, contact us to obtain a release under GPL Version 2 or 3 or a different license than the AGPL.
****

include::../dedication.adoc[]

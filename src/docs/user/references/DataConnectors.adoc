
The `data-connectors` bucket contains diverse generic data connectors for the most frequently used data storage systems (File, SQL, NoSQL, Cloud, ERP). The aim of a data connector is to facilitate and simplify data access from your workflows to external data sources.

=== File
The File connectors allow to import and export data from HTTP, FTP and SFTP servers.
We begin by presenting FTP and SFTP connectors then, URL connector.

==== FTP and SFTP connectors

*Variables:*

The FTP and SFTP connectors share the same list of variables. Consequently, we describe them in the following table using a unique notation.
`<PROTOCOL>` can take one of the following values: {`FTP`, `SFTP`}

.FTP/SFTP Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `<PROTOCOL>_HOSTNAME`
| IP address of the server host.
| Workflow, Task
| Yes
| String
| `localhost`
| `<PROTOCOL>_PORT`
| Listening port.
| Workflow, Task
| No
| Integer
| e.g. 21, 22
| `<PROTOCOL>_LOCAL_RELATIVE_PATH`
| Local `relative` path from which we upload (or to which we download) file(s).
It can contain either a path to a file, a directory terminated by `/` or an empty value for the root.
| Workflow, Task
| No
| String
| e.g. localDirectory/, example.zip
| `<PROTOCOL>_REMOTE_RELATIVE_PATH`
| Remote `relative` path to which we upload (or from which we download) file(s).
| Workflow, Task
| Yes
| String
| e.g. remoteDirectory/, test.txt
| `<PROTOCOL>_MODE`
| Transfer mode.
| Workflow, Task
| Yes
| PA:LIST(GET, PUT)
| GET
| `<PROTOCOL>_EXTRACT_ARCHIVE`
| Used only when `<PROTOCOL>_MODE=GET`. If set to `true`, the imported file will be extracted if it is an archive.
| Workflow, Task
| Yes
|  Boolean [true or false]
| false
| `<PROTOCOL>_USERNAME`
| Username to use for the authentication.
| Workflow, Task
| Yes
|  String
| e.g. \ftp://someuser@example.com
|===

*How to use this task:*

The task requires the following third-party credential: `{key: <PROTOCOL>://<username>@<hostname>, value: <PROTOCOL>_PASSWORD}`. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

==== URL connector

The URL connector allows, in addition, to import data using HTTP and HTTPS protocols.

*Variables:*

.URL Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `FILE_URL`
| Link to a file accessible using HTTP, HTTPS, SFTP or FTP protocols.

  FTP and SFTP urls must have the following patterns:

- `\ftp://<username>[:<password>]@<hostname>[:<port>]/<relativePath>`

- `sftp://<username>[:<password>]@<hostname>[:<port>]/<relativePath>`

| Task
| Yes
| String
| e.g. sftp://user:pass@example.com/test.txt
| `FTP_LOCAL_RELATIVE_PATH`
|  Local relative path from which we upload (or to which we download) file(s).
  LOCAL_RELATIVE_PATH can contain either a path to a file, a directory terminated by `/` or an empty value for the root.
| Task
| No
| String
| e.g. localDirectory/, example.zip
| `EXTRACT_ARCHIVE`
| If set to `true`, the imported file will be extracted if it is an archive.
| Task
| Yes
| Boolean [true or false]
| false
|===

*How to use this task:*

We highly recommend the user to not provide his password within the URL and to instead use the third-party credential mechanism as follows: `{key: <PROTOCOL>://<username>@<hostname>, value: <PROTOCOL>_PASSWORD}`. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

=== SQL
The SQL connectors allow to import and export data from and to Relational DataBase Management Systems (RDBMS).
Currently, we have connectors for Mysql, Oracle, Postgres, Greenplum and Sql server.

SQL connectors workflows are composed of two tasks: an import task and an export task.



*Variables:*

All SQL connectors share the same list of variables. Consequently, we describe them in the following table using a unique notation.
`<RDBMS_NAME>` can take one of the following values: {`POSTGRES`, `GPDB`, `ORACLE`, `SQL_SERVER`, `MYSQL`}

.SQL Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `<RDBMS_NAME>_HOSTNAME`
| Label or the IP address of the server hosting the database.
| Workflow
| Yes
| String
| `localhost`
| `<RDBMS_NAME>_PORT`
| Listening port.
| Workflow
| No
| Integer
| e.g. 5432, 1521
| `<RDBMS_NAME>_DATABASE`
| Database name.
| Workflow
| Yes
| String
| e.g. MY_DATABASE
| `<RDBMS_NAME>_QUERY`
| Requires an SQL query or a table name to fetch data from.
| Import Task
| Yes
| String
| e.g.

```SELECT * from MY_TABLE```
| `LABEL`
| If the imported data is labeled for machine learning, a label attribute can be specified using this variable.
| Import Task
| No
| String
| e.g. class
| `<RDBMS_NAME>_OUTPUT_FILE`
| Relative path in the data space used to save the results in a CSV file.
| Import Task
| No
| String
| e.g. `path/to/my/output.csv`
| `<RDBMS_NAME>_TABLE`
| The table to insert data into.
| Export Task
| Yes
| String
| e.g. MY_TABLE
| `INSERT_MODE`
a| Indicates the behavior to follow when the table exists in the database amongst:

  * fail: If table exists, do nothing.
  * replace: If table exists, drop it, recreate it, and insert data.
  * append: (default) If table exists, insert data. Create if does not exist.

| Export Task
| Yes
| String
| _
| `INPUT_FILE`
a| * It can be a relative path in the dataspace of a csv file containing the data to import.
* or a valid URL amongst `http`, `ftp`, `s3`, and `file`.
| Export Task
| Yes
| String
| e.g. `path/to/data.csv` or
http://link/to/my/data/csv
| `<RDBMS_NAME>_RMDB_DRIVER`
| The driver to connect to the database.
| Import Task
Export Task
| Yes
| String
| e.g. cx_oracle, psycopg2
|===

*How to use this task:*

When you drag & drop an SQL connector, two tasks will be appended to your workflow: an import task and an export task. You can keep one of them depending on your needs and remove the other or you can use them both.

This task uses the driver given in `RMDB_DRIVER` to connect to the database. To use another driver, make sure you have it properly installed before (e.g. using `pip install <RMDBS_DRIVER>`).

The task requires the following third-party credentials: `<RDBMS_NAME>_USERNAME` and `<RDBMS_NAME>_PASSWORD` ; this is a one-time action and will ensure that your credentials are securely encrypted. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

In the import mode, the output containing the imported data takes one or many of the following forms:

* in a _CSV_ format to saved to:
 ** the `<RDBMS_NAME>_OUTPUT_FILE` in the data space if specified by the user
 ** _and_ to the `result` variable to make is previewable in the scheduler portal and to make it accessible for the next tasks.
* in a _JSON_ format using the variable `DATAFRAME_JSON`.

=== NoSQL
The NoSQL connectors allow to import data from NoSQL Databases.
Currently, we have connectors for MongoDB and Cassandra.

==== MongoDB

*Variables:*

.NoSQL Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `MONGODB_HOSTNAME`
| Label or the IP address of the server hosting the database.
| Workflow, Import Task, Export Task
| Yes
| String
| `localhost`
| `MONGODB_PORT`
| Listening port.
| Workflow, Import Task, Export Task
| No
| Integer
| 27018
| `MONGODB_DATABASE`
| Database to use. In export mode, it is created if it does not exist.
| Workflow, Import Task, Export Task
| Yes
| String
| e.g. my_database
| `MONGODB_COLLECTION`
| Collection to use. In export mode, it is created if it does not exist.
| Workflow, Import Task, Export Task
| Yes
| String
| e.g. my_collection
| `MONGODB_QUERY`
| Requires a NoSQL query to fetch data. If empty (`{}`), it will fetch all documents.
| Import Task
| No
| String
| {}
| `MONGODB_OUTPUT`
| Relative path in the data space used to save the results in a JSON file.
| Import Task
| No
| String
| e.g. path/to/my/output.json
| `LABEL`
| If the imported data is labeled for machine learning, a label attribute can be specified using this variable.
| Import Task
| No
| String
| e.g. class

| `MONGODB_INPUT`
a| A JSON Object/Array to be inserted in MongoDB. This variable can:

   * A String describing the JSON Object/Array
   * A relative path in the data space of a JSON file.
| Export Task
| Yes
| String
| e.g.

`{"document":{"key":"value"}}`

or `path/to/input.json`
|===

*How to use this task:*

The task requires the following third-party credentials: `MONGODB_USERNAME` and `MONGODB_PASSWORD`. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

In the import mode, the output containing the imported data takes one or many of the following forms:

* in a JSON format to saved to:
 ** the `MONGODB_OUTPUT` file in the data space if specified by the user
 ** _and_ to the `result` variable to make is previewable in the scheduler portal and to make it accessible for the next tasks.

=== Cloud

Cloud data connectors allow to interact with cloud storage services. Currently we provide support for Amazon S3, Azure Storage and Azure Data Lake.

==== Azure Data Lake

The Azure Data Lake connector allows to upload U-SQL scripts and then execute them as Data Lake Analytics (DLA) jobs. It requires an existing Data Lake Analytics account and its corresponding Data Lake Store account. The connector workflow consists of three tasks:

* _Submit_job_: Connects to Azure Data Lake and submits the provided script.
* _Wait_for_job_: Periodically monitors the DLA job status until its finalization.
* _Display_result_: Downloads the result file and displays it.

*Variables:*

.Azure Data Lake Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?* | *Type*  | *Default/Examples*
| `AZ_DLA_ACCOUNT`
| Data Lake *Analytics* account to be used. It should already exist.
| Workflow
| Yes
| String
| e.g. my_dla_account
| `AZ_DLS_ACCOUNT`
| Data Lake *Store* account to be used.  It should already exist.
| Workflow
| Yes
| String
| e.g. my_dls_account
| `AZ_DLA_JOB`
| Name to identify the job to be submitted.
| Workflow
| Yes
| String
| e.g. my_dla_job
| `AZ_DLA_SCRIPT`
| File name of the U-SQL script to submit. The file must be located in the *Global Space* directory. An example file `script.usql` is provided.
| Workflow
| Yes
| String
v|Sample file: script.usql
e.g. my_usql_script.usql
| `AZ_DLA_OUTPUT`
| Name of the output file to store the result of the script.
| Workflow
| Yes
| String
| e.g. my_output_file.csv
| `AZ_CRON_MONITOR`
| Cron expression to determine how frequently to monitor the completion of the job.
| Workflow
| Yes
| String
v|Default: "* * * * \*"
(every minute)
e.g. "*/2 * * * *"
(every 2 minutes)
|===

*How to use these tasks:*

Azure Data Lake tasks require your Azure login credentials to be set as third-party credentials (`key:value` pairs); this is a one-time action and will ensure that your credentials are securely encrypted. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

You have two options for providing your login credentials:

* Standard Azure login: `AZ_U:your_user` (usually an email). `AZ_P:your_password`.
* Using an link:https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?toc=%2Fazure%2Fazure-resource-manager%2Ftoc.json&view=azure-cli-latest[Azure service principal]: `AZ_U:appId`. `AZ_P:password`. `AZ_T:tenant`. By default, if `AZ_T` is set, the tasks will attempt to connect through a service principal.

[NOTE]
====
.The Output File
* Instead of hardcoding the name of your output file in your U-SQL script, you can use the placeholder `OUTPUT_FILE`, which is automatically replaced by the value of `AZ_DLA_OUTPUT`.
* Once downloaded, the output file will be stored in your *User Space* (and _not_ in the Global Space).
====
